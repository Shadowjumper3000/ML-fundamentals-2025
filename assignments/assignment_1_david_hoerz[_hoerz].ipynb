{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Introduction and Summary of Assignment\n",
    "\n",
    "In this assignment, we will be working with the Titanic dataset. The goal is to load, preprocess, and analyze the data to gain insights into the factors that influenced the survival of passengers. We will perform various data preprocessing steps such as handling missing values, encoding categorical variables, and feature scaling.\n",
    "\n",
    "## Loading and Preprocessing of the Titanic Dataset\n",
    "\n",
    "We will start by loading the Titanic dataset and performing necessary preprocessing steps to prepare the data for analysis. This includes:\n",
    "\n",
    "1. Handling missing values.\n",
    "2. Encoding categorical variables.\n",
    "3. Feature scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Dataset Description\n",
    "\n",
    "1. **survival**: Survival (0 = No; 1 = Yes).\n",
    "2. **class**: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd).\n",
    "3. **name**: Name.\n",
    "4. **sex**: Sex.\n",
    "5. **sibsp**: Number of Siblings/Spouses Aboard.\n",
    "6. **parch**: Number of Parents/Children Aboard.\n",
    "7. **ticket**: Ticket Number.\n",
    "8. **fare**: Passenger Fare.\n",
    "9. **cabin**: Cabin.\n",
    "10. **embarked**: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton).\n",
    "11. **boat**: Lifeboat (if survived).\n",
    "12. **body**: Body number (if did not survive and the body was recovered).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boruta\n",
    "%pip install imblearn\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install scipy\n",
    "%pip install statsmodels\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from boruta import BorutaPy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic data loading and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = 'titanic3.xls'\n",
    "titanic_df = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(titanic_df.head())\n",
    "\n",
    "# Display the dataset information\n",
    "print(titanic_df.info())\n",
    "\n",
    "# Display the summary statistics of the dataset\n",
    "print(titanic_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Managing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating and Showing the Percentage of Missing Values in Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for each column\n",
    "missing_values_count = titanic_df.isnull().sum()\n",
    "print(\"\\nMissing Values Count for Each Column:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Filter out rows with missing values\n",
    "filtered_df = titanic_df.dropna()\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(\"\\nFiltered Dataset (No Missing Values):\")\n",
    "print(filtered_df.head())\n",
    "\n",
    "# Display the dataset information\n",
    "print(\"\\nFiltered Dataset Info:\")\n",
    "print(filtered_df.info())\n",
    "\n",
    "# Display the summary statistics of the filtered dataset\n",
    "print(\"\\nFiltered Dataset Summary Statistics:\")\n",
    "print(filtered_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Number of People Survived/!Survived depending on Boat/!Boat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Filter the dataset for people who have a boat value and also survived\n",
    "boat_and_survived = titanic_df[(titanic_df['boat'].notnull()) & (titanic_df['survived'] == 1)]\n",
    "\n",
    "# Calculate the number of people who have a boat value and also survived\n",
    "num_boat_and_survived = boat_and_survived.shape[0]\n",
    "\n",
    "print(f\"Number of people who have a boat value and also survived: {num_boat_and_survived}\")\n",
    "\n",
    "# Filter the dataset for people who have a boat value and didn't survive\n",
    "boat_and_not_survived = titanic_df[(titanic_df['boat'].notnull()) & (titanic_df['survived'] == 0)]\n",
    "\n",
    "# Calculate the number of people who have a boat value and didn't survive\n",
    "num_boat_and_not_survived = boat_and_not_survived.shape[0]\n",
    "\n",
    "print(f\"Number of people who have a boat value and didn't survive: {num_boat_and_not_survived}\")\n",
    "\n",
    "# Calculate the number of people who didn't have a boat value and survived\n",
    "no_boat_and_survived = titanic_df[(titanic_df['boat'].isnull()) & (titanic_df['survived'] == 1)]\n",
    "num_no_boat_and_survived = no_boat_and_survived.shape[0]\n",
    "\n",
    "print(f\"Number of people who didn't have a boat value and survived: {num_no_boat_and_survived}\")\n",
    "\n",
    "# Calculate the number of people who didn't have a boat value and didn't survive\n",
    "no_boat_and_not_survived = titanic_df[(titanic_df['boat'].isnull()) & (titanic_df['survived'] == 0)]\n",
    "num_no_boat_and_not_survived = no_boat_and_not_survived.shape[0]\n",
    "\n",
    "print(f\"Number of people who didn't have a boat value and didn't survive: {num_no_boat_and_not_survived}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting 2x2 Matrix of Survived/!Survived depending on Boat/!Boat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Create a 2x2 matrix with the data\n",
    "matrix = [\n",
    "    [num_boat_and_survived, num_boat_and_not_survived],\n",
    "    [num_no_boat_and_survived, num_no_boat_and_not_survived]\n",
    "]\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\", xticklabels=['Boat', 'No Boat'], yticklabels=['Survived', 'Not Survived'])\n",
    "plt.title('Survival vs Boat Presence')\n",
    "plt.xlabel('Boat Presence')\n",
    "plt.ylabel('Survival Status')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Boat Column has high correlation with Survival status, I will be dropping it as I consider it future information/data leakage. As the Boat Status is dependent on the other factors, aswell as it being a future information, I will be dropping it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'boat' column\n",
    "titanic_df.drop(columns=['boat'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Body Column will also be dropped, as it is future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'body' column, as this is dependent on survival status\n",
    "titanic_df.drop(columns=['body'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although a Name such as Dr. or Ms. could have an impact on the survival rate, I will be dropping the Name Column as it is not a feature that can be used in the model. It would add complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'name' column\n",
    "titanic_df.drop(columns=['name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ticket Column will also be dropped as it is not a feature that can be used in the model. It would add complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'ticket' column as it is not statistically significant\n",
    "titanic_df.drop(columns=['ticket'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embarked Column will be dropped, as it adds complexity and is theoretically not a feature that can be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'embarked' column\n",
    "titanic_df.drop(columns=['embarked'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Home destination would not be a feature that can be used in the model, so I will be dropping the Home.dest Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'home.dest' column\n",
    "titanic_df.drop(columns=['home.dest'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the Median Age for each sex will make sure that the Age Column is not biased. I will be filling the missing values with the median age. As the standard deviation of the Age is high, which is why the mean would not be a good choice, as it is skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median age for each sex\n",
    "median_age_per_sex = titanic_df.groupby('sex')['age'].median()\n",
    "\n",
    "# Function to fill missing age values based on sex\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        return median_age_per_sex[row['sex']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# Apply the function to fill missing age values\n",
    "titanic_df['age'] = titanic_df.apply(fill_age, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be calculating the average fare price per class and filling the missing values with the average fare price per class. This approach ensures that the missing fare values are imputed based on the average fare paid by passengers in their respective classes, which is a reasonable assumption given that fare prices are often correlated with passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the average fare for each class, excluding fares of 0 or N/A\n",
    "average_fare_per_class = titanic_df[titanic_df['fare'] > 0].groupby('pclass')['fare'].mean()\n",
    "\n",
    "# Fill missing fare values with the average fare of their respective class\n",
    "titanic_df['fare'] = titanic_df.apply(\n",
    "    lambda row: average_fare_per_class[row['pclass']] if pd.isnull(row['fare']) or row['fare'] == 0 else row['fare'],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the median Fare for Cabins, I will reverse engineer the Cabin Column and fill the missing values with the median Fare price per Cabin. This approach ensures that the missing cabin values are imputed based on the fare paid by the passengers, which is a reasonable assumption given that fare prices are often correlated with cabin assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median fare for each cabin\n",
    "titanic_df['cabin'] = titanic_df['cabin'].str[0]  # Extract the first letter of the cabin\n",
    "average_fare_per_cabin = titanic_df.groupby('cabin')['fare'].median().to_dict()\n",
    "\n",
    "# Plot the distribution of cabin decks based on the first letter\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=titanic_df, x='cabin', order=sorted(titanic_df['cabin'].dropna().unique()))\n",
    "plt.title('Distribution of Cabin Decks')\n",
    "plt.xlabel('Cabin Deck')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Function to assign cabin based on fare price\n",
    "def assign_cabin(fare):\n",
    "    for cabin, median_fare in average_fare_per_cabin.items():\n",
    "        if fare <= median_fare:\n",
    "            return cabin\n",
    "    return 'T'  # Assign 'T' if fare is higher than all median fares\n",
    "\n",
    "# Fill missing cabin values based on fare price\n",
    "titanic_df['cabin'] = titanic_df.apply(\n",
    "    lambda row: assign_cabin(row['fare']) if pd.isnull(row['cabin']) else row['cabin'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Plot the distribution of cabin decks after filling missing values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=titanic_df, x='cabin', order=sorted(titanic_df['cabin'].unique()))\n",
    "plt.title('Distribution of Cabin Decks After Filling Missing Values')\n",
    "plt.xlabel('Cabin Deck')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing that there are no missing values left in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated dataset information\n",
    "print(titanic_df.info(verbose=True))\n",
    "\n",
    "# Show columns with missing values\n",
    "columns_with_missing_values = titanic_df.columns[titanic_df.isnull().any()]\n",
    "print(\"\\nColumns with Missing Values:\")\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will perform one-hot encoding on the Passenger Class, because it is a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for the 'pclass' column\n",
    "titanic_df = pd.get_dummies(titanic_df, columns=['pclass'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will perform one-hot encoding on the Passenger Gender, as it is a categorical variable. This process involves converting the gender categories into a format that can be provided to machine learning algorithms to improve the model's performance. By doing this, we ensure that the gender information is represented numerically, allowing the model to interpret and utilize this feature effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding for the 'sex' column\n",
    "titanic_df = pd.get_dummies(titanic_df, columns=['sex'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_df.columns)\n",
    "print(titanic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization will be performed to see if the data is normally distributed. If it is not, I will perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the 'fare' column\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(titanic_df['fare'], kde=True)\n",
    "plt.title('Histogram of Fare')\n",
    "\n",
    "# Plot Q-Q plot of the 'fare' column\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(titanic_df['fare'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Fare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Standard Scale Fare Column, as it is a continuous variable. This Scaled version will be used in the model, as it will be easier to interpret for the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Perform standardized scaling for the 'fare' column\n",
    "titanic_df['fare_scaled'] = scaler.fit_transform(titanic_df[['fare']])\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "print(titanic_df[['fare', 'fare_scaled']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plot a Q-Q plot to check if the Fare Column is normally distributed. I will be using the Scaled version of the Fare Column in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram and Q-Q plot of the 'fare_scaled' column\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(titanic_df['fare_scaled'], kde=True)\n",
    "plt.title('Histogram of Scaled Fare')\n",
    "plt.xlabel('Scaled Fare')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Q-Q plot\n",
    "plt.subplot(1, 2, 2)\n",
    "stats.probplot(titanic_df['fare_scaled'], dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Scaled Fare')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Scale the Age using MinMaxScaler, as it is a continuous variable. This Scaled version will be used in the model, as it will be easier to interpret for the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Perform MinMax scaling for the 'age' column\n",
    "titanic_df['age_scaled'] = minmax_scaler.fit_transform(titanic_df[['age']])\n",
    "\n",
    "# Display the first few rows of the updated dataset\n",
    "print(titanic_df[['age', 'age_scaled']].head())\n",
    "\n",
    "# Drop the 'age' column as it is now scaled\n",
    "titanic_df.drop(columns=['age'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_df.columns)\n",
    "print(titanic_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be splitting the dataset into three parts: training, testing, and validation sets. The training set will consist of 50% of the data, the testing set will consist of 30% of the data, and the validation set will consist of 20% of the data. This approach ensures that the model is trained on a substantial portion of the data, while also having separate sets for testing and validation to evaluate the model's performance and generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data to convert categorical features to numeric values\n",
    "X = pd.get_dummies(titanic_df.drop(columns=['survived']))\n",
    "y = titanic_df['survived']\n",
    "\n",
    "# Split the dataset into training (50%) and temp (50%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Split the temp set into validation (60% of temp, which is 30% of original) and testing (40% of temp, which is 20% of original) sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42)\n",
    "\n",
    "# Display the sizes of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "# Function to plot pie chart for survived/!survived\n",
    "def plot_survival_pie_chart(data, title, ax):\n",
    "    survival_counts = data.value_counts()\n",
    "    ax.pie(survival_counts, labels=['Not Survived', 'Survived'], autopct='%1.1f%%', startangle=140, colors=['#ff9999','#66b3ff'])\n",
    "    ax.set_title(title)\n",
    "    ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot pie charts for each dataset\n",
    "plot_survival_pie_chart(y_train, 'Training Data Split: Survived vs Not Survived', axes[0])\n",
    "plot_survival_pie_chart(y_val, 'Validation Data Split: Survived vs Not Survived', axes[1])\n",
    "plot_survival_pie_chart(y_test, 'Testing Data Split: Survived vs Not Survived', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Adressing Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this dataset is imbalanced, I will be using SMOTE to balance the dataset. The training data will be balanced, while the testing and validation sets will remain the same. This approach ensures that the model is trained on a balanced dataset, which helps in improving the model's performance and generalization. However, the testing and validation sets will remain imbalanced to reflect the real-world scenario and to evaluate the model's performance accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of survivors and non-survivors in the training set\n",
    "survival_counts = y_train.value_counts()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot pie charts for each dataset\n",
    "plot_survival_pie_chart(y_train, 'Training Data Split: Survived vs Not Survived', axes[0])\n",
    "plot_survival_pie_chart(y_val, 'Validation Data Split: Survived vs Not Survived', axes[1])\n",
    "plot_survival_pie_chart(y_test, 'Testing Data Split: Survived vs Not Survived', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training set\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display the sizes of the original and resampled training sets\n",
    "print(f\"Original training set size: {X_train.shape[0]}\")\n",
    "print(f\"Resampled training set size: {X_train_smote.shape[0]}\")\n",
    "\n",
    "# Display the distribution of the target variable in the resampled training set\n",
    "print(\"\\nDistribution of the target variable in the resampled training set:\")\n",
    "print(y_train_smote.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Class Imbalance before and after SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of survivors and non-survivors in the new dataset\n",
    "survival_counts_smote = y_train_smote.value_counts()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot pie charts for each dataset\n",
    "plot_survival_pie_chart(y_train_smote, 'Training Data Split (SMOTE): Survived vs Not Survived', axes[0])\n",
    "plot_survival_pie_chart(y_val, 'Validation Data Split: Survived vs Not Survived', axes[1])\n",
    "plot_survival_pie_chart(y_test, 'Testing Data Split: Survived vs Not Survived', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_smote.columns)\n",
    "print(X_train_smote.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the Boruta algorithm to select the most relevant features for the model. This method helps in identifying the important features by iteratively removing the least important ones, ensuring that only the most significant features are retained for the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "boruta_selector.fit(X_train_smote.values, y_train_smote.values)\n",
    "\n",
    "selected_features = X_train_smote.columns[boruta_selector.support_].to_list()\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used GridSearchCV to find the best hyperparameters for my Logistic Regression model. This helps me optimize the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8:Traning a Logistic Regression Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using simple Logistic Regression Model to predict the survival of passengers based on the selected features. I will evaluate the model's performance using accuracy, precision, recall, and F1-score metrics on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%pip install joblib\n",
    "import joblib\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "lr = LogisticRegression(max_iter=3000, random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit GridSearchCV on the training set with selected features\n",
    "grid_search.fit(X_train_smote[selected_features], y_train_smote)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Train the logistic regression model with the best hyperparameters\n",
    "best_lr = LogisticRegression(**best_params, max_iter=3000, random_state=42)\n",
    "best_lr.fit(X_train_smote[selected_features], y_train_smote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluated my model on the validation set to check its performance. I looked at accuracy, the confusion matrix, and the classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_val_pred = best_lr.predict(X_val[selected_features])\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Validation Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display the classification report\n",
    "val_cr = classification_report(y_val, y_val_pred)\n",
    "print(\"Validation Classification Report:\")\n",
    "print(val_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combined the training and validation sets to retrain my model with the best hyperparameters. This helps improve the model by using more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the training and validation sets\n",
    "X_train_combined = pd.concat([X_train_smote, X_val])\n",
    "y_train_combined = pd.concat([y_train_smote, y_val])\n",
    "\n",
    "# Retrain the logistic regression model on the combined dataset with the best hyperparameters\n",
    "final_lr = LogisticRegression(**best_params, max_iter=3000, random_state=42)\n",
    "final_lr.fit(X_train_combined[selected_features], y_train_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluated my final model on the testing set to see how well it performs on completely unseen data. I checked the accuracy, confusion matrix, and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_test_pred = final_lr.predict(X_test[selected_features])\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "\n",
    "# Display the confusion matrix\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Testing Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Display the classification report\n",
    "test_cr = classification_report(y_test, y_test_pred)\n",
    "print(\"Testing Classification Report:\")\n",
    "print(test_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Conclusion, the model has an accuracy of 0.75 , I consider this a good result, as the model is able to predict the survival of passengers with a high degree of accuracy. The precision, recall, and F1-score metrics are also good, indicating that the model is performing well in terms of predicting the survival of passengers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
