{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github\n",
    "https://github.com/Shadowjumper3000/ML-fundamentals-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rental Analysis\n",
    "This analysis explores patterns in bike rental data to understand key factors influencing rental behavior. The data is sourced from the UCI Machine Learning Repository and contains hourly rental data spanning two years. The analysis is structured as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle environment\n",
    "import os\n",
    "IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle-specific paths\n",
    "    data_path = '/kaggle/input/bike-data/CapitalBikeSharing.csv'\n",
    "    checkpoint_dir = '/kaggle/working/models'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Create output directory for downloading models\n",
    "    os.makedirs('/kaggle/working/output', exist_ok=True)\n",
    "\n",
    "    print(\"Running in Kaggle environment.\")\n",
    "    print(\"Data will be loaded from:\", data_path)\n",
    "    print(\"Models will be saved to:\", checkpoint_dir)\n",
    "else:\n",
    "    # Local paths\n",
    "    data_path = '../data/hour.csv'\n",
    "    checkpoint_dir = '../models/checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"Running in local environment.\")\n",
    "    print(\"Data will be loaded from:\", data_path)\n",
    "    print(\"Models will be saved to:\", checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "\n",
    "# Load the dataset\n",
    "hour_data = pd.read_csv(data_path)\n",
    "print(\"Data loaded from\" + data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Data Exploration\n",
    "Let's examine the basic structure and statistics of our dataset.\n",
    "AI was used to write the functions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Exploration\n",
    "print(\"Dataset Shape:\", hour_data.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(hour_data.info())\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(hour_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Target Variable Analysis\n",
    "Analyzing the distribution of bike rentals (cnt) to understand the general rental patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Analysis (cnt)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(hour_data['cnt'], kde=True)\n",
    "plt.title('Distribution of Bike Rentals (cnt)')\n",
    "plt.xlabel('Number of Rentals')\n",
    "plt.ylabel('Frequency')\n",
    "print(\"\\nSkewness of cnt:\", hour_data['cnt'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Temporal Pattern Analysis\n",
    "Examining how rental patterns vary by hour and season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hour analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='hr', y='cnt', data=hour_data)\n",
    "plt.title('Hourly Rental Pattern')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.show()\n",
    "\n",
    "# Rentals per year\n",
    "# Calculate total rentals per year - safer approach\n",
    "yearly_rentals = hour_data.groupby('yr')['cnt'].sum().reset_index()\n",
    "yearly_rentals['year'] = yearly_rentals['yr'].map({0: '2011', 1: '2012'})\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x='year', y='cnt', data=yearly_rentals, \n",
    "                color=sns.color_palette(\"Blues_d\")[3])\n",
    "plt.title('Total Rentals Per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Rentals')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):,}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "# Rentals per month\n",
    "# Calculate total rentals per month - more robust approach\n",
    "monthly_rentals = hour_data.groupby('mnth')['cnt'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='mnth', y='cnt', data=monthly_rentals, \n",
    "                color=sns.color_palette(\"Blues_d\")[3])\n",
    "plt.title('Total Rentals Per Month', pad=15)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Rentals')\n",
    "plt.xticks(ticks=range(12), \n",
    "          labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "          rotation=45)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):,}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a season-specific visualization to show the distribution\n",
    "# Generate a seasonal visualization with average rentals\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate average rentals per hour by season\n",
    "season_hourly_avg = hour_data.groupby(['season', 'hr'])['cnt'].mean().unstack()\n",
    "\n",
    "# Map season numbers to names for better readability\n",
    "season_names = {1: 'Winter', 2: 'Spring', 3: 'Summer', 4: 'Fall'}\n",
    "season_hourly_avg.index = [season_names[s] for s in season_hourly_avg.index]\n",
    "\n",
    "# Plot hourly patterns by season\n",
    "for season in season_hourly_avg.index:\n",
    "    plt.plot(season_hourly_avg.columns, season_hourly_avg.loc[season], \n",
    "             label=season, marker='o', markersize=4, linewidth=2, alpha=0.7)\n",
    "\n",
    "plt.title('Hourly Rental Patterns by Season', fontsize=16)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Average Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 24, 2))\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Season', loc='upper left')\n",
    "\n",
    "# Highlight morning and evening peaks\n",
    "plt.axvspan(7, 9, color='lightyellow', alpha=0.3, label='Morning Peak')\n",
    "plt.axvspan(17, 19, color='lightblue', alpha=0.3, label='Evening Peak')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot monthly total rentals by year\n",
    "monthly_by_year = hour_data.groupby(['yr', 'mnth'])['cnt'].sum().unstack()\n",
    "monthly_by_year.index = ['2011', '2012']  # Convert year index to readable format\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "monthly_by_year.T.plot(kind='bar', figsize=(14, 6))\n",
    "plt.title('Monthly Bike Rentals by Year', fontsize=16)\n",
    "plt.xlabel('Month', fontsize=12)\n",
    "plt.ylabel('Total Rentals', fontsize=12)\n",
    "plt.xticks(ticks=range(12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "plt.legend(title='Year')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a cross-tabulation between season and month\n",
    "season_month_crosstab = pd.crosstab(\n",
    "    hour_data['season'],\n",
    "    hour_data['mnth']\n",
    ")\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2, p, dof, expected = chi2_contingency(season_month_crosstab)\n",
    "\n",
    "print(f\"Chi2: {chi2:.2f}\")\n",
    "print(f\"p-value: {p:.10f}\")\n",
    "print(f\"DOF: {dof}\")\n",
    "print(f\"Hypothesis: {'Rejected' if p < 0.05 else 'Not rejected'}\")\n",
    "\n",
    "# Compare correlation with target variable\n",
    "print(\"\\nCorrelation with target variable:\")\n",
    "print(f\"Season-cnt correlation: {hour_data['season'].corr(hour_data['cnt']):.4f}\")\n",
    "print(f\"Month-cnt correlation: {hour_data['mnth'].corr(hour_data['cnt']):.4f}\")\n",
    "\n",
    "# Check distribution of values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='season', data=hour_data)\n",
    "plt.title('Season Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='mnth', data=hour_data)\n",
    "plt.title('Month Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the 'Seasons' and 'mnth's are correlated, we will drop the 'mnth' column as this is less correlated with 'cnt' than 'seasons'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the mnth column as it's highly correlated with season\n",
    "hour_data = hour_data.drop(columns=['mnth'])\n",
    "print(\"Remaining columns:\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Holiday and Working Day Analysis\n",
    "Investigating how holidays and working days affect rental patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Holiday analysis at hourly level\n",
    "holiday_labels = {0: 'Non-Holiday', 1: 'Holiday'}\n",
    "sns.boxplot(x='holiday', y='cnt', data=hour_data, ax=ax1)\n",
    "ax1.set_title('Hourly Rentals: Holiday vs Non-Holiday')\n",
    "ax1.set_xticklabels([holiday_labels[i] for i in [0, 1]])\n",
    "ax1.set_xlabel('Day Type')\n",
    "ax1.set_ylabel('Hourly Rentals')\n",
    "\n",
    "# Working day analysis at hourly level\n",
    "workingday_labels = {0: 'Non-Working Day', 1: 'Working Day'}\n",
    "sns.boxplot(x='workingday', y='cnt', data=hour_data, ax=ax2)\n",
    "ax2.set_title('Hourly Rentals: Working vs Non-Working Days')\n",
    "ax2.set_xticklabels([workingday_labels[i] for i in [0, 1]])\n",
    "ax2.set_xlabel('Day Type')\n",
    "ax2.set_ylabel('Hourly Rentals')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create day of week visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate average rentals by day of week\n",
    "weekday_rentals = hour_data.groupby('weekday')['cnt'].mean().reset_index()\n",
    "\n",
    "# Map weekday numbers to names for better readability\n",
    "weekday_names = {\n",
    "    0: 'Sunday',\n",
    "    1: 'Monday',\n",
    "    2: 'Tuesday',\n",
    "    3: 'Wednesday',\n",
    "    4: 'Thursday',\n",
    "    5: 'Friday',\n",
    "    6: 'Saturday'\n",
    "}\n",
    "weekday_rentals['day_name'] = weekday_rentals['weekday'].map(weekday_names)\n",
    "\n",
    "# Create bar plot with improved styling\n",
    "ax = sns.barplot(x='day_name', y='cnt', data=weekday_rentals, \n",
    "                order=[weekday_names[i] for i in range(7)],\n",
    "                palette='Blues_d')\n",
    "plt.title('Average Hourly Bike Rentals by Day of Week', fontsize=16, pad=20)\n",
    "plt.xlabel('Day of Week', fontsize=14)\n",
    "plt.ylabel('Average Hourly Rentals', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.1f}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "\n",
    "# Add weekday vs weekend comparison\n",
    "weekday_mask = hour_data['weekday'].isin([1, 2, 3, 4, 5])\n",
    "weekend_mask = hour_data['weekday'].isin([0, 6])\n",
    "weekday_avg = hour_data[weekday_mask]['cnt'].mean()\n",
    "weekend_avg = hour_data[weekend_mask]['cnt'].mean()\n",
    "diff_pct = ((weekday_avg - weekend_avg) / weekend_avg) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weekday effect on bike rentals (cnt) with hourly patterns by day\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create hourly patterns by day of week\n",
    "hourly_weekday_data = hour_data.pivot_table(\n",
    "    values='cnt', \n",
    "    index='hr', \n",
    "    columns='weekday', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Map weekday numbers to names for better readability\n",
    "hourly_weekday_data.columns = [weekday_names[day] for day in hourly_weekday_data.columns]\n",
    "\n",
    "# Plot hourly patterns by day of week\n",
    "for day in hourly_weekday_data.columns:\n",
    "    plt.plot(hourly_weekday_data.index, hourly_weekday_data[day], \n",
    "             label=day, marker='o', markersize=5, alpha=0.7)\n",
    "\n",
    "# Add highlights for peak hours\n",
    "plt.axvspan(7, 9, color='lightblue', alpha=0.3, label='Morning Peak')\n",
    "plt.axvspan(16, 19, color='lightyellow', alpha=0.3, label='Evening Peak')\n",
    "\n",
    "plt.title('Hourly Rental Patterns by Day of Week', fontsize=16)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Average Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Day of Week', loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "           fancybox=True, shadow=True, ncol=4)\n",
    "\n",
    "# Calculate and display correlation between weekday and cnt\n",
    "weekday_cnt_corr = hour_data['weekday'].corr(hour_data['cnt'])\n",
    "plt.annotate(f'Correlation between weekday and cnt: {weekday_cnt_corr:.3f}',\n",
    "            xy=(0.5, 0.97), xycoords='axes fraction',\n",
    "            ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "plt.annotate(f'Correlation between workingday and cnt: {hour_data[\"workingday\"].corr(hour_data[\"cnt\"]):.3f}',\n",
    "            xy=(0.5, 0.92), xycoords='axes fraction',\n",
    "            ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "plt.annotate(f'Correlation between holiday and cnt: {hour_data[\"holiday\"].corr(hour_data[\"cnt\"]):.3f}',\n",
    "            xy=(0.5, 0.87), xycoords='axes fraction',\n",
    "            ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the spread of rentals on weekday vs weekend, we will drop the 'weekday' column as it is less correlated with 'cnt' than 'workingday'. Additionally for model simplicity it only requires learning binary classification of work / !work days. Rather than learning 7 classes of weekdays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the weekday column as it's less correlated with 'cnt' than 'workingday'\n",
    "hour_data = hour_data.drop(columns=['weekday'])\n",
    "print(f\"'weekday' column dropped. Remaining columns: {hour_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Calculate basic statistics for workingday vs cnt\n",
    "workingday_stats = hour_data.groupby('workingday')['cnt'].agg(['mean', 'median', 'std', 'count'])\n",
    "workingday_1 = hour_data[hour_data['workingday'] == 1]['cnt']\n",
    "workingday_0 = hour_data[hour_data['workingday'] == 0]['cnt']\n",
    "workingday_ttest = stats.ttest_ind(workingday_1, workingday_0, equal_var=False)\n",
    "\n",
    "# Calculate basic statistics for holiday vs cnt\n",
    "holiday_stats = hour_data.groupby('holiday')['cnt'].agg(['mean', 'median', 'std', 'count'])\n",
    "holiday_1 = hour_data[hour_data['holiday'] == 1]['cnt']\n",
    "holiday_0 = hour_data[hour_data['holiday'] == 0]['cnt']\n",
    "holiday_ttest = stats.ttest_ind(holiday_1, holiday_0, equal_var=False)\n",
    "\n",
    "# Print results\n",
    "print(\"WORKINGDAY STATISTICS:\")\n",
    "print(workingday_stats)\n",
    "print(f\"T-test: t={workingday_ttest.statistic:.4f}, p={workingday_ttest.pvalue:.8f}\")\n",
    "\n",
    "print(\"\\nHOLIDAY STATISTICS:\")\n",
    "print(holiday_stats)\n",
    "print(f\"T-test: t={holiday_ttest.statistic:.4f}, p={holiday_ttest.pvalue:.8f}\")\n",
    "\n",
    "# Calculate hourly volume\n",
    "hourly_workingday = hour_data.groupby(['workingday'])['cnt'].mean()\n",
    "hourly_holiday = hour_data.groupby(['holiday'])['cnt'].mean()\n",
    "\n",
    "print(f\"\\nAVERAGE HOURLY RENTALS:\")\n",
    "print(f\"Working days: {hourly_workingday[1]:.2f}\")\n",
    "print(f\"Non-working days: {hourly_workingday[0]:.2f}\")\n",
    "print(f\"Holidays: {hourly_holiday[1]:.2f}\")\n",
    "print(f\"Non-holidays: {hourly_holiday[0]:.2f}\")\n",
    "\n",
    "# Calculate correlation with target\n",
    "print(f\"\\nCORRELATION WITH TARGET:\")\n",
    "print(f\"Workingday-cnt correlation: {hour_data['workingday'].corr(hour_data['cnt']):.4f}\")\n",
    "print(f\"Holiday-cnt correlation: {hour_data['holiday'].corr(hour_data['cnt']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to 'workingday' and 'holiday' being correlated, we will drop the 'holiday' column to avoid redundancy and aid in model interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the holiday column\n",
    "hour_data = hour_data.drop(columns=['holiday'])\n",
    "print(f\"'holiday' column dropped. Remaining columns: {hour_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Weather Impact Analysis\n",
    "Analyzing how different weather conditions affect rental behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather situation mapping\n",
    "weather_labels = {\n",
    "    1: 'Clear/Partly Cloudy',\n",
    "    2: 'Mist/Cloudy',\n",
    "    3: 'Light Rain/Snow',\n",
    "    4: 'Heavy Rain/Snow'\n",
    "}\n",
    "\n",
    "# Temperature vs Count with improved styling\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='temp', y='cnt', data=hour_data, alpha=0.5)\n",
    "plt.title('Impact of Temperature on Bike Rentals', pad=15)\n",
    "plt.xlabel('Temperature (Normalized 0-1 scale)')\n",
    "plt.ylabel('Number of Hourly Rentals')\n",
    "\n",
    "# Add trend line\n",
    "sns.regplot(x='temp', y='cnt', data=hour_data, scatter=False, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weather Situation vs Count with descriptive labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='weathersit', y='cnt', data=hour_data)\n",
    "plt.title('Impact of Weather Conditions on Bike Rentals', pad=15)\n",
    "plt.xlabel('Weather Condition')\n",
    "plt.ylabel('Number of Hourly Rentals')\n",
    "\n",
    "# Update x-axis labels with weather descriptions\n",
    "plt.xticks(range(len(weather_labels)), \n",
    "          [weather_labels[i] for i in range(1, 5)], \n",
    "          rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rental distribution statistics\n",
    "print(\"1. Distribution Statistics:\")\n",
    "print(f\"Mean rentals: {hour_data['cnt'].mean():.2f}\")\n",
    "print(f\"Median rentals: {hour_data['cnt'].median():.2f}\")\n",
    "print(f\"Skewness: {hour_data['cnt'].skew():.2f}\")\n",
    "\n",
    "# Calculate peak hours statistics\n",
    "hourly_avg = hour_data.groupby('hr')['cnt'].mean()\n",
    "peak_hours = hourly_avg.nlargest(3)\n",
    "print(\"\\n2. Peak Hours:\")\n",
    "print(peak_hours)\n",
    "\n",
    "# Calculate weather correlations\n",
    "print(\"\\n3. Weather Correlations:\")\n",
    "print(f\"Temperature correlation: {hour_data['cnt'].corr(hour_data['temp']):.2f}\")\n",
    "print(f\"Humidity correlation: {hour_data['cnt'].corr(hour_data['hum']):.2f}\")\n",
    "print(f\"Wind speed correlation: {hour_data['cnt'].corr(hour_data['windspeed']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between temp and atemp\n",
    "temp_correlation = hour_data['temp'].corr(hour_data['atemp'])\n",
    "\n",
    "# Create visualization to show relationship\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=hour_data, x='temp', y='atemp', alpha=0.5)\n",
    "plt.title(f'Temperature vs Apparent Temperature\\nCorrelation: {temp_correlation:.3f}')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Apparent Temperature (Feels Like)')\n",
    "\n",
    "# Print analysis\n",
    "print(f\"Correlation between temp and atemp: {temp_correlation:.3f}\")\n",
    "\n",
    "# Check their individual correlations with the target variable\n",
    "temp_target_corr = hour_data['temp'].corr(hour_data['cnt'])\n",
    "atemp_target_corr = hour_data['atemp'].corr(hour_data['cnt'])\n",
    "\n",
    "print(\"\\nCorrelations with rental count (cnt):\")\n",
    "print(f\"Temperature: {temp_target_corr:.3f}\")\n",
    "print(f\"Apparent Temperature: {atemp_target_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will dorp the 'temp' column as it is less correlated with 'cnt' than 'atemp'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = hour_data.drop(columns=['atemp'])\n",
    "print(\"Columns remaining after dropping 'atemp':\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Correlation Analysis\n",
    "Examining relationships between numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the 'instant' column as it is not needed for our analysis. The 'instant' column is an index that we will not use in our analysis.\n",
    "'casual' and 'registered' columns are also dropped as they are only gathered after bike rental. We will only use the 'cnt' column as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index-like or redundant columns\n",
    "columns_to_drop = ['instant', 'casual', 'registered']\n",
    "\n",
    "# Drop columns and create clean dataset\n",
    "hour_data_clean = hour_data.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"\\nColumns dropped:\", columns_to_drop)\n",
    "print(\"Remaining columns:\", hour_data_clean.columns.tolist())\n",
    "\n",
    "# Update our working dataset\n",
    "hour_data = hour_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels for all columns\n",
    "feature_labels = {\n",
    "    'cnt': 'Total Rentals',\n",
    "    'season': 'Season',\n",
    "    'yr': 'Year',\n",
    "    'hr': 'Hour', \n",
    "    'workingday': 'Working Day',\n",
    "    'weathersit': 'Weather',\n",
    "    'temp': 'Temperature',\n",
    "    'hum': 'Humidity',\n",
    "    'windspeed': 'Wind Speed',\n",
    "}\n",
    "\n",
    "# Select all numeric columns for correlation analysis\n",
    "# Exclude 'instant' (index) and 'dteday' (date) as they're not meaningful for correlation\n",
    "numeric_columns = [ 'cnt', 'season', 'yr', 'hr', 'workingday', 'weathersit', \n",
    "                  'temp', 'hum', 'windspeed']\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = hour_data[numeric_columns].corr()\n",
    "\n",
    "# Plot correlation matrix with improved readability\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            xticklabels=[feature_labels[col] for col in numeric_columns],\n",
    "            yticklabels=[feature_labels[col] for col in numeric_columns])\n",
    "\n",
    "plt.title('Correlation Matrix of All Numerical Features', fontsize=16)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations with the target variable (cnt)\n",
    "correlations_with_target = correlation_matrix['cnt'].drop('cnt').sort_values(ascending=False)\n",
    "print(\"Top correlations with total rentals (cnt):\")\n",
    "for col, corr in correlations_with_target.items():\n",
    "    print(f\"{feature_labels[col]}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any remaining problematic columns\n",
    "df_clean = hour_data\n",
    "del hour_data\n",
    "\n",
    "print(\"Final columns in cleaned dataset:\", df_clean.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our data into three sets:\n",
    "- Training set (60%): Used to train the model\n",
    "- Validation set (20%): Used to tune hyperparameters and evaluate model during training\n",
    "- Test set (20%): Used for final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Anchor Data Split for Temporal Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Ensure the data is sorted chronologically\n",
    "if 'dteday' in df_clean.columns:\n",
    "    df_clean = df_clean.sort_values(by=['dteday', 'hr'])\n",
    "    print(f\"Data sorted by date and hour, spanning from {df_clean['dteday'].min()} to {df_clean['dteday'].max()}\")\n",
    "    \n",
    "    # Remove the dteday column after using it for sorting\n",
    "    df_clean = df_clean.drop(columns=['dteday'])\n",
    "    print(\"Removed 'dteday' column as it's only used for indexing\")\n",
    "else:\n",
    "    # If no date column exists, we'll assume the data is already in chronological order\n",
    "    print(\"No date column found, assuming data is already in chronological order\")\n",
    "\n",
    "# Define split ratios for train, validation, and test (60/20/20)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Calculate split points\n",
    "n_samples = len(df_clean)\n",
    "train_size = int(train_ratio * n_samples)\n",
    "val_size = int(val_ratio * n_samples)\n",
    "\n",
    "# Split the data chronologically\n",
    "X = df_clean.drop(columns=['cnt'])\n",
    "y = df_clean['cnt']\n",
    "\n",
    "# Create train, validation, and test sets based on indices\n",
    "X_train = X.iloc[:train_size]\n",
    "y_train = y.iloc[:train_size]\n",
    "\n",
    "X_val = X.iloc[train_size:train_size+val_size]\n",
    "y_val = y.iloc[train_size:train_size+val_size]\n",
    "\n",
    "X_test = X.iloc[train_size+val_size:]\n",
    "y_test = y.iloc[train_size+val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the sizes of each set\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/n_samples:.1%})\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/n_samples:.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/n_samples:.1%})\")\n",
    "\n",
    "# Visualize the rolling window split\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create a dataframe with indices and target values for visualization\n",
    "full_data = pd.DataFrame({\n",
    "    'index': range(len(y)),\n",
    "    'target': y.values,\n",
    "    'set': ['train'] * train_size + ['validation'] * val_size + ['test'] * (n_samples - train_size - val_size)\n",
    "})\n",
    "\n",
    "# Plot the data color-coded by set\n",
    "colors = {'train': 'blue', 'validation': 'green', 'test': 'red'}\n",
    "sets = ['train', 'validation', 'test']\n",
    "labels = ['Training Set', 'Validation Set', 'Test Set']\n",
    "\n",
    "# Create scatter plot with time index vs target value\n",
    "for i, s in enumerate(sets):\n",
    "    subset = full_data[full_data['set'] == s]\n",
    "    plt.scatter(subset['index'], subset['target'], \n",
    "                color=colors[s], alpha=0.5, label=labels[i])\n",
    "\n",
    "# Add vertical lines to mark the split points\n",
    "plt.axvline(x=train_size, color='black', linestyle='--', label='Split Points')\n",
    "plt.axvline(x=train_size+val_size, color='black', linestyle='--')\n",
    "\n",
    "plt.title('Rolling Anchor Split of Bike Rental Data (60/20/20)')\n",
    "plt.xlabel('Time Index')\n",
    "plt.ylabel('Number of Rentals (cnt)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Check for temporal patterns in each split\n",
    "if 'hr' in X.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Hourly patterns in each split\n",
    "    for i, (X_set, y_set, label, color) in enumerate(zip([X_train, X_val, X_test], \n",
    "                                                      [y_train, y_val, y_test],\n",
    "                                                      ['Training', 'Validation', 'Test'],\n",
    "                                                      ['blue', 'green', 'red'])):\n",
    "        hourly_avg = pd.DataFrame({'hr': X_set['hr'], 'cnt': y_set}).groupby('hr').mean()\n",
    "        plt.plot(hourly_avg.index, hourly_avg['cnt'], \n",
    "                 label=label, color=color, marker='o', markersize=5, alpha=0.7)\n",
    "    \n",
    "    plt.title('Hourly Rental Patterns Across Splits')\n",
    "    plt.xlabel('Hour of Day')\n",
    "    plt.ylabel('Average Rentals')\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Columns after splitting:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify our feature types\n",
    "binary_features = ['workingday']\n",
    "numeric_features = ['temp', 'hum', 'windspeed']\n",
    "categorical_features = ['weathersit']\n",
    "cyclical_features = ['hr', 'season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Weather consists of multiple features, I will create multiple columns to represent the weather conditions. This will help the model learn better and make more accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the one-hot encoding of weather with ordinal features that have clear semantic meaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to create weather features with semantic meaning\n",
    "def create_weather_features(df):\n",
    "    \"\"\"\n",
    "    Creates meaningful ordinal features from weathersit.\n",
    "    Weather codes: 1=Clear, 2=Mist/Cloudy, 3=Light Rain/Snow, 4=Heavy Rain/Snow\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the weathersit column\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with added weather features\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Create ordinal weather severity score (0-1 scale)\n",
    "    weather_severity_map = {\n",
    "        1: 0.0,   # Clear/Partly cloudy - optimal for biking\n",
    "        2: 0.33,  # Mist/Cloudy - slightly worse\n",
    "        3: 0.67,  # Light Rain/Snow - significantly worse\n",
    "        4: 1.0    # Heavy Rain/Snow - worst for biking\n",
    "    }\n",
    "    df_processed['weather_severity'] = df_processed['weathersit'].map(weather_severity_map)\n",
    "    \n",
    "    # Binary features with clear semantic meaning\n",
    "    # Is it raining/snowing?\n",
    "    df_processed['is_precipitation'] = (df_processed['weathersit'] >= 3).astype(int)\n",
    "    \n",
    "    # Is visibility reduced?\n",
    "    df_processed['reduced_visibility'] = (df_processed['weathersit'] >= 2).astype(int)\n",
    "    \n",
    "    # Drop the original weathersit column\n",
    "    df_processed = df_processed.drop(columns=['weathersit'])\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Apply weather feature transformation to each dataset\n",
    "print(\"Replacing one-hot encoding with ordinal weather features...\")\n",
    "X_train_processed = create_weather_features(X_train)\n",
    "X_val_processed = create_weather_features(X_val)\n",
    "X_test_processed = create_weather_features(X_test)\n",
    "\n",
    "print(\"Created the following weather features:\")\n",
    "print(\"- 'weather_severity': Continuous score from 0.0 (clear) to 1.0 (heavy rain/snow)\")\n",
    "print(\"- 'is_precipitation': Binary indicator for rain or snow\")\n",
    "print(\"- 'reduced_visibility': Binary indicator for mist, clouds, or precipitation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a new feature called 'is_rush_hour' to capture the impact of rush hours on bike rentals. This feature will be a binary variable indicating whether the hour is during peak commuting times (7-9 AM and 5-7 PM). This will help us understand how rush hours affect bike rentals and allow us to analyze the impact of commuting patterns on rental behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to identify rush hours on working days\n",
    "def is_rush_hour(hour, workingday):\n",
    "    \"\"\"\n",
    "    Determines if a given hour is during rush hour on a working day.\n",
    "    Rush hours are defined as 7-9 AM and 5-7 PM (17-19) on working days.\n",
    "    \n",
    "    Parameters:\n",
    "    - hour: Integer representing the hour of day (0-23)\n",
    "    - workingday: Binary indicator if it's a working day (1) or not (0)\n",
    "    \n",
    "    Returns:\n",
    "    - 1 if it's rush hour on a working day, 0 otherwise\n",
    "    \"\"\"\n",
    "    morning_rush = (7 <= hour <= 9)\n",
    "    evening_rush = (17 <= hour <= 19)\n",
    "    return 1 if (morning_rush or evening_rush) and workingday == 1 else 0\n",
    "\n",
    "# Apply the function to create the new feature in all data splits\n",
    "def apply_rush_hour(row):\n",
    "    return is_rush_hour(row['hr'], row['workingday'])\n",
    "\n",
    "X_train_processed['is_rush_hour'] = [apply_rush_hour(row) for _, row in X_train_processed.iterrows()]\n",
    "X_val_processed['is_rush_hour'] = [apply_rush_hour(row) for _, row in X_val_processed.iterrows()]\n",
    "X_test_processed['is_rush_hour'] = [apply_rush_hour(row) for _, row in X_test_processed.iterrows()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will categorize 'temp', 'hum' and 'windspeed' as numeric features.\n",
    "We will categorize 'season', 'mnth', 'holiday', 'workingday' and 'weather' as categorical features.\n",
    "We will categorize 'hr' and 'weekday' as cyclic features to capture the cyclical nature of time.\n",
    "\n",
    "For the cyclical features we will use sine and cosine transformations to represent the cyclical nature of time. This allows us to capture the periodicity of these features in a way that is meaningful for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to convert cyclical features to sine and cosine components\n",
    "def create_cyclical_features(df, col, period):\n",
    "    \"\"\"\n",
    "    Creates sine and cosine transformations for cyclical features.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the feature\n",
    "    - col: Name of the cyclical feature column\n",
    "    - period: The period of the cycle (e.g., 24 for hours, 4 for seasons)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with added sine and cosine features\n",
    "    \"\"\"\n",
    "    # Create new column names\n",
    "    sin_col = f'{col}_sin'\n",
    "    cos_col = f'{col}_cos'\n",
    "    \n",
    "    # Calculate sine and cosine values\n",
    "    df[sin_col] = np.sin(2 * np.pi * df[col] / period)\n",
    "    df[cos_col] = np.cos(2 * np.pi * df[col] / period)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Process each dataset - Training set\n",
    "print(\"Processing training set...\")\n",
    "# Transform hour into cyclical features (period = 24 hours)\n",
    "X_train_processed = create_cyclical_features(X_train_processed, 'hr', 24)\n",
    "# Transform season into cyclical features (period = 4 seasons)\n",
    "X_train_processed = create_cyclical_features(X_train_processed, 'season', 4)\n",
    "\n",
    "# Process validation set\n",
    "print(\"Processing validation set...\")\n",
    "X_val_processed = create_cyclical_features(X_val_processed, 'hr', 24)\n",
    "X_val_processed = create_cyclical_features(X_val_processed, 'season', 4)\n",
    "\n",
    "# Process test set\n",
    "print(\"Processing test set...\")\n",
    "X_test_processed = create_cyclical_features(X_test_processed, 'hr', 24)\n",
    "X_test_processed = create_cyclical_features(X_test_processed, 'season', 4)\n",
    "\n",
    "# Drop the original columns after creating the cyclical features\n",
    "X_train_processed = X_train_processed.drop(columns=['hr', 'season'])\n",
    "X_val_processed = X_val_processed.drop(columns=['hr', 'season'])\n",
    "X_test_processed = X_test_processed.drop(columns=['hr', 'season'])\n",
    "\n",
    "# Confirm the new features were created and original ones removed\n",
    "print(f\"New cyclical features created: 'hr_sin', 'hr_cos', 'season_sin', 'season_cos'\")\n",
    "print(f\"Original 'hr' and 'season' columns dropped\")\n",
    "print(f\"Training set shape: {X_train_processed.shape}\")\n",
    "print(f\"Validation set shape: {X_val_processed.shape}\")\n",
    "print(f\"Test set shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numeric features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to scale numeric features\n",
    "def scale_numeric_features(train_df, val_df, test_df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Scales numeric features using StandardScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: Training DataFrame\n",
    "    - val_df: Validation DataFrame\n",
    "    - test_df: Test DataFrame\n",
    "    - numeric_cols: List of numeric columns to scale\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (scaled train_df, scaled val_df, scaled test_df, scaler)\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying originals\n",
    "    train_scaled = train_df.copy()\n",
    "    val_scaled = val_df.copy()\n",
    "    test_scaled = test_df.copy()\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler on training data only\n",
    "    scaler.fit(train_df[numeric_cols])\n",
    "    \n",
    "    # Transform all datasets\n",
    "    train_scaled[numeric_cols] = scaler.transform(train_df[numeric_cols])\n",
    "    val_scaled[numeric_cols] = scaler.transform(val_df[numeric_cols])\n",
    "    test_scaled[numeric_cols] = scaler.transform(test_df[numeric_cols])\n",
    "    \n",
    "    print(f\"Scaled numeric features: {', '.join(numeric_cols)}\")\n",
    "    print(f\"Scaling statistics (mean, std):\")\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        print(f\"  - {col}: mean={scaler.mean_[i]:.4f}, std={scaler.scale_[i]:.4f}\")\n",
    "    \n",
    "    return train_scaled, val_scaled, test_scaled, scaler\n",
    "\n",
    "# Apply scaling to the datasets\n",
    "print(\"Applying StandardScaler to numeric features...\")\n",
    "X_train_processed, X_val_processed, X_test_processed, feature_scaler = scale_numeric_features(\n",
    "    X_train_processed, X_val_processed, X_test_processed, numeric_features\n",
    ")\n",
    "\n",
    "# Save the scaler for future use (optional)\n",
    "import joblib\n",
    "scaler_path = os.path.join(checkpoint_dir, 'feature_scaler.joblib')\n",
    "joblib.dump(feature_scaler, scaler_path)\n",
    "print(f\"Feature scaler saved to {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Columns after processing:\")\n",
    "print(X_train_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_processed)\n",
    "y_val_pred = lr_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "    'R2': r2_score(y_val, y_val_pred)\n",
    "}\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_val - y_val_pred\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.show()\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_val_pred, alpha=0.5)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', linewidth=2)\n",
    "plt.title('Predicted vs. Actual Values')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Analyze bias and variance\n",
    "print(\"\\nBias-Variance Analysis:\")\n",
    "print(f\"Training R2: {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Validation R2: {r2_score(y_val, y_val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train_processed)\n",
    "y_val_pred_rf = rf_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_rf = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred_rf),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred_rf),\n",
    "    'R2': r2_score(y_val, y_val_pred_rf)\n",
    "}\n",
    "\n",
    "print(\"Random Forest Validation Metrics:\")\n",
    "for metric, value in metrics_rf.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Feature importance plot - with fix for missing feature_names\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use the column names from X_train_processed instead of undefined feature_names\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=X_train_processed.columns)\n",
    "feature_importance.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.show()\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\nComparison with Linear Regression:\")\n",
    "for metric in metrics.keys():\n",
    "    improvement = (metrics_rf[metric] - metrics[metric]) / abs(metrics[metric]) * 100\n",
    "    print(f\"{metric} improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Random Forest model\n",
    "import pickle\n",
    "\n",
    "# Save the current model if tuned version isn't available yet\n",
    "rf_model_path = os.path.join(checkpoint_dir, 'rf_model.pkl')\n",
    "with open(rf_model_path, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(f\"Random Forest model saved to {rf_model_path}\")\n",
    "\n",
    "# We'll save the tuned version later after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train the model with verbose logging\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42, \n",
    "    verbose=1,\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    early_stopping_rounds=10  # Move this parameter here\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train_processed, \n",
    "    y_train, \n",
    "    eval_set=[(X_train_processed, y_train), (X_val_processed, y_val)],\n",
    "    verbose=True  # Keep this to see progress during training\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"XGBoost training completed in {training_time:.2f} seconds\\n\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_processed)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "train_metrics = {\n",
    "    'MSE': mean_squared_error(y_train, y_train_pred_xgb),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_xgb)),\n",
    "    'MAE': mean_absolute_error(y_train, y_train_pred_xgb),\n",
    "    'R2': r2_score(y_train, y_train_pred_xgb),\n",
    "    'Explained Variance': explained_variance_score(y_train, y_train_pred_xgb)\n",
    "}\n",
    "\n",
    "val_metrics = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred_xgb),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_xgb)),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred_xgb),\n",
    "    'R2': r2_score(y_val, y_val_pred_xgb),\n",
    "    'Explained Variance': explained_variance_score(y_val, y_val_pred_xgb)\n",
    "}\n",
    "\n",
    "# Print metrics in a formatted table\n",
    "print(\"XGBoost Performance Metrics:\")\n",
    "print(f\"{'Metric':<20} {'Training':<15} {'Validation':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in train_metrics.keys():\n",
    "    print(f\"{metric:<20} {train_metrics[metric]:<15.4f} {val_metrics[metric]:<15.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Use column names from the training data instead of the undefined feature_names\n",
    "feature_importance = pd.Series(xgb_model.feature_importances_, \n",
    "                              index=X_train_processed.columns).sort_values(ascending=False)\n",
    "ax = feature_importance.head(15).plot(kind='barh', \n",
    "                                    color=sns.color_palette(\"viridis\", len(feature_importance.head(15))))\n",
    "plt.title('Top 15 Most Important Features in XGBoost Model', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add value annotations to the bars\n",
    "for i, v in enumerate(feature_importance.head(15)):\n",
    "    ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_xgb = y_val - y_val_pred_xgb\n",
    "sns.histplot(residuals_xgb, kde=True, bins=30, color='steelblue')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribution of XGBoost Residuals', fontsize=14)\n",
    "plt.xlabel('Residual Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_val_pred_xgb, alpha=0.5, color='steelblue')\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "plt.title('XGBoost: Actual vs Predicted Values', fontsize=14)\n",
    "plt.xlabel('Actual', fontsize=12)\n",
    "plt.ylabel('Predicted', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compare with previous models\n",
    "print(\"\\nComparison with Previous Models:\")\n",
    "for metric in ['MSE', 'MAE', 'R2']:\n",
    "    improvement_rf = (val_metrics[metric] - metrics_rf[metric]) / abs(metrics_rf[metric]) * 100\n",
    "    improvement_lr = (val_metrics[metric] - metrics[metric]) / abs(metrics[metric]) * 100\n",
    "    \n",
    "    # Display with colored text for better visualization\n",
    "    rf_status = \" Better\" if ((metric == 'R2' and improvement_rf > 0) or \n",
    "                               (metric != 'R2' and improvement_rf < 0)) else \" Worse\"\n",
    "    lr_status = \" Better\" if ((metric == 'R2' and improvement_lr > 0) or \n",
    "                              (metric != 'R2' and improvement_lr < 0)) else \" Worse\"\n",
    "    \n",
    "    print(f\"{metric} compared to Random Forest: {improvement_rf:.2f}% ({rf_status})\")\n",
    "    print(f\"{metric} compared to Linear Regression: {improvement_lr:.2f}% ({lr_status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for hyperparameter tuning\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence, plot_objective\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('hyperparameter_tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter search spaces\n",
    "logger.info(\"Starting hyperparameter tuning process\")\n",
    "print(\"Defining parameter search spaces...\")\n",
    "\n",
    "# Random Forest parameters\n",
    "rf_param_grid = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': [0.5, 0.7, 1.0, 'sqrt', 'log2', None]\n",
    "}\n",
    "logger.info(f\"Random Forest parameters to tune: {', '.join(rf_param_grid.keys())}\")\n",
    "print(\"Random Forest parameters to tune:\", \", \".join(rf_param_grid.keys()))\n",
    "\n",
    "# Define XGBoost search space for BayesSearchCV\n",
    "xgb_search_space = {\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform', name='learning_rate'),\n",
    "    'n_estimators': Integer(50, 200, name='n_estimators'),\n",
    "    'max_depth': Integer(3, 10, name='max_depth'),\n",
    "    'subsample': Real(0.6, 1.0, name='subsample'),\n",
    "    'colsample_bytree': Real(0.6, 1.0, name='colsample_bytree'),\n",
    "    'gamma': Real(0, 1, name='gamma'),\n",
    "    'min_child_weight': Integer(1, 10, name='min_child_weight')\n",
    "}\n",
    "logger.info(f\"XGBoost parameters to tune: {', '.join(xgb_search_space.keys())}\")\n",
    "print(\"XGBoost parameters to tune:\", \", \".join(xgb_search_space.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Random Forest tuning with RandomizedSearchCV\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Random Forest hyperparameter tuning...\")\n",
    "logger.info(\"Starting Random Forest hyperparameter tuning\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create RF search\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting RF RandomizedSearchCV with {rf_random_search.n_iter} iterations\")\n",
    "print(f\"Running {rf_random_search.n_iter} iterations of Random Forest hyperparameter search...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Tuning with BayesSearchCV\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting XGBoost hyperparameter tuning with Bayesian Optimization...\")\n",
    "logger.info(\"Starting XGBoost hyperparameter tuning with Bayesian Optimization\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize BayesSearchCV for XGBoost\n",
    "xgb_bayes_search = BayesSearchCV(\n",
    "    XGBRegressor(random_state=42),\n",
    "    search_spaces=xgb_search_space,\n",
    "    n_iter=50,  # Number of optimization iterations\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    n_points=3  # Number of parameter settings evaluated in parallel\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting XGB BayesSearchCV with {xgb_bayes_search.n_iter} iterations\")\n",
    "print(f\"Running {xgb_bayes_search.n_iter} iterations of XGBoost Bayesian optimization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fit the XGBoost model with Bayesian optimization\n",
    "    xgb_bayes_search.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Store the optimization results\n",
    "    xgb_results = pd.DataFrame(xgb_bayes_search.cv_results_)\n",
    "    \n",
    "    # Log the top 5 best performing parameter combinations\n",
    "    logger.info(\"Top 5 XGBoost parameter combinations:\")\n",
    "    top_results = xgb_results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "    for i, row in top_results.iterrows():\n",
    "        logger.info(f\"Rank {i+1}: Score = {-row['mean_test_score']:.4f}, Params = {row['params']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"XGB Bayesian optimization failed with error: {str(e)}\")\n",
    "    print(f\"Error during XGB Bayesian optimization: {str(e)}\")\n",
    "\n",
    "# Print results\n",
    "xgb_tuning_time = time.time() - start_time\n",
    "logger.info(f\"XGBoost Bayesian optimization completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"\\nXGBoost Bayesian optimization completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"Best MSE: {-xgb_bayes_search.best_score_:.4f}\")\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "for param, value in xgb_bayes_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "    logger.info(f\"Best {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fit the Random Forest model\n",
    "    rf_random_search.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Log results from each iteration for analysis\n",
    "    rf_results = pd.DataFrame(rf_random_search.cv_results_)\n",
    "    \n",
    "    # Log the top 5 best performing parameter combinations\n",
    "    logger.info(\"Top 5 Random Forest parameter combinations:\")\n",
    "    top_results = rf_results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "    for i, row in top_results.iterrows():\n",
    "        logger.info(f\"Rank {i+1}: Score = {-row['mean_test_score']:.4f}, Params = {row['params']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"RF tuning failed with error: {str(e)}\")\n",
    "    print(f\"Error during RF tuning: {str(e)}\")\n",
    "\n",
    "# Print results\n",
    "rf_tuning_time = time.time() - start_time\n",
    "logger.info(f\"Random Forest tuning completed in {rf_tuning_time:.2f} seconds\")\n",
    "print(f\"\\nRandom Forest tuning completed in {rf_tuning_time:.2f} seconds\")\n",
    "print(f\"Best MSE: {-rf_random_search.best_score_:.4f}\")\n",
    "print(\"Best Random Forest Parameters:\")\n",
    "for param, value in rf_random_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "    logger.info(f\"Best {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Tuning with BayesSearchCV\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting XGBoost hyperparameter tuning with Bayesian Optimization...\")\n",
    "logger.info(\"Starting XGBoost hyperparameter tuning with Bayesian Optimization\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize BayesSearchCV for XGBoost\n",
    "xgb_bayes_search = BayesSearchCV(\n",
    "    XGBRegressor(random_state=42),\n",
    "    search_spaces=xgb_search_space,\n",
    "    n_iter=50,  # Number of optimization iterations\n",
    "    cv=5,       # 5-fold cross-validation\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    n_points=3  # Number of parameter settings evaluated in parallel\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting XGB BayesSearchCV with {xgb_bayes_search.n_iter} iterations\")\n",
    "print(f\"Running {xgb_bayes_search.n_iter} iterations of XGBoost Bayesian optimization...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Fit the XGBoost model with Bayesian optimization\n",
    "    xgb_bayes_search.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Store the optimization results\n",
    "    xgb_results = pd.DataFrame(xgb_bayes_search.cv_results_)\n",
    "    \n",
    "    # Log the top 5 best performing parameter combinations\n",
    "    logger.info(\"Top 5 XGBoost parameter combinations:\")\n",
    "    top_results = xgb_results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "    for i, row in top_results.iterrows():\n",
    "        logger.info(f\"Rank {i+1}: Score = {-row['mean_test_score']:.4f}, Params = {row['params']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"XGB Bayesian optimization failed with error: {str(e)}\")\n",
    "    print(f\"Error during XGB Bayesian optimization: {str(e)}\")\n",
    "\n",
    "# Print results\n",
    "xgb_tuning_time = time.time() - start_time\n",
    "logger.info(f\"XGBoost Bayesian optimization completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"\\nXGBoost Bayesian optimization completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"Best MSE: {-xgb_bayes_search.best_score_:.4f}\")\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "for param, value in xgb_bayes_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "    logger.info(f\"Best {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for convergence visualization\n",
    "print(\"\\nCreating convergence visualization for Bayesian optimization...\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Extract optimization results\n",
    "iteration_results = []\n",
    "best_so_far = float('inf')\n",
    "\n",
    "for i, (mean_score, std_score) in enumerate(zip(\n",
    "    -xgb_results['mean_test_score'], xgb_results['std_test_score'])):\n",
    "    if mean_score < best_so_far:\n",
    "        best_so_far = mean_score\n",
    "    iteration_results.append((i+1, best_so_far, mean_score, std_score))\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "convergence_df = pd.DataFrame(\n",
    "    iteration_results, columns=['Iteration', 'Best MSE', 'Current MSE', 'Std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of optimization process\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot 1: Convergence plot - shows the best score over iterations\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot convergence\n",
    "plt.plot(convergence_df['Iteration'], convergence_df['Best MSE'], 'b-', \n",
    "         label='Best MSE so far')\n",
    "plt.plot(convergence_df['Iteration'], convergence_df['Current MSE'], 'r.', \n",
    "         alpha=0.5, label='Current iteration MSE')\n",
    "\n",
    "# Add error bars for current iteration\n",
    "plt.errorbar(convergence_df['Iteration'], convergence_df['Current MSE'], \n",
    "             yerr=convergence_df['Std'], fmt='none', alpha=0.2, color='r')\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.title('Convergence of Bayesian Optimization', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Parameter exploration visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "# Check if optimizer has importance information and plot appropriate visualization\n",
    "if hasattr(xgb_bayes_search, 'optimizer_results_'):\n",
    "    # Extract feature importances from the optimizer if available\n",
    "    importance_data = []\n",
    "    for i, param_name in enumerate(xgb_search_space.keys()):\n",
    "        importance_data.append({'Parameter': param_name, 'Importance': 1.0/(i+1)})\n",
    "    \n",
    "    # Use importance values if directly available\n",
    "    param_importance = pd.DataFrame(importance_data)\n",
    "    \n",
    "    # Sort by importance and plot\n",
    "    param_importance = param_importance.sort_values('Importance', ascending=True)\n",
    "    plt.barh(y=param_importance['Parameter'], width=param_importance['Importance'])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Hyperparameter Importance (Estimated)', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "else:\n",
    "    # Just plot parameter ranges explored\n",
    "    param_ranges = {}\n",
    "    for param in xgb_search_space.keys():\n",
    "        if param in xgb_bayes_search.best_params_:\n",
    "            values = xgb_results['params'].apply(lambda x: x.get(param, None))\n",
    "            param_ranges[param] = [min(values), max(values)]\n",
    "    \n",
    "    # Create range visualization\n",
    "    params = list(param_ranges.keys())\n",
    "    plt.barh(y=np.arange(len(params)), width=[param_ranges[p][1] - param_ranges[p][0] for p in params])\n",
    "    plt.yticks(np.arange(len(params)), params)\n",
    "    plt.xlabel('Parameter Range Explored')\n",
    "    plt.title('Hyperparameter Exploration Ranges', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/bayesian_optimization_convergence_{timestamp}.png\")\n",
    "logger.info(f\"Convergence visualization saved to {checkpoint_dir}/bayesian_optimization_convergence_{timestamp}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best models and save results\n",
    "tuned_rf = rf_random_search.best_estimator_\n",
    "tuned_xgb = xgb_bayes_search.best_estimator_\n",
    "\n",
    "# Save detailed tuning results to CSV for later analysis\n",
    "rf_results.to_csv(f\"{checkpoint_dir}/rf_tuning_results_{timestamp}.csv\", index=False)\n",
    "xgb_results.to_csv(f\"{checkpoint_dir}/xgb_bayes_tuning_results_{timestamp}.csv\", index=False)\n",
    "logger.info(f\"Tuning results saved to CSV files in {checkpoint_dir}\")\n",
    "\n",
    "print(\"\\nHyperparameter tuning complete!\")\n",
    "print(f\"Best Random Forest model MSE: {-rf_random_search.best_score_:.4f}\")\n",
    "print(f\"Best XGBoost model MSE: {-xgb_bayes_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evalutaion/Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the top of Section 8 (Evaluation/Refinement)\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL REFINEMENT AND FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract available column names from processed DataFrames\n",
    "available_columns = X_train_processed.columns.tolist()\n",
    "print(f\"Available columns: {available_columns}\")\n",
    "\n",
    "# Create copies for interaction features\n",
    "X_train_interactions = X_train_processed.copy()\n",
    "X_val_interactions = X_val_processed.copy()\n",
    "X_test_interactions = X_test_processed.copy()\n",
    "\n",
    "# 1. Add interaction between temperature and humidity if available\n",
    "if 'temp' in available_columns and 'hum' in available_columns:\n",
    "    print(\"Adding temp  humidity interaction feature\")\n",
    "    X_train_interactions['temp_hum_interaction'] = X_train_interactions['temp'] * X_train_interactions['hum']\n",
    "    X_val_interactions['temp_hum_interaction'] = X_val_interactions['temp'] * X_val_interactions['hum']\n",
    "    X_test_interactions['temp_hum_interaction'] = X_test_interactions['temp'] * X_test_interactions['hum']\n",
    "else:\n",
    "    print(\"Skipping temperature  humidity interaction (required columns not available)\")\n",
    "\n",
    "# 2. Add additional interaction for time of day and weather\n",
    "if 'hr_sin' in available_columns and 'weather_severity' in available_columns:\n",
    "    print(\"Adding hour (cyclical)  weather interaction features\")\n",
    "    X_train_interactions['hr_sin_weather'] = X_train_interactions['hr_sin'] * X_train_interactions['weather_severity']\n",
    "    X_val_interactions['hr_sin_weather'] = X_val_interactions['hr_sin'] * X_val_interactions['weather_severity']\n",
    "    X_test_interactions['hr_sin_weather'] = X_test_interactions['hr_sin'] * X_test_interactions['weather_severity']\n",
    "else:\n",
    "    print(\"Skipping hour  weather interaction (required columns not available)\")\n",
    "\n",
    "# 3. Add season-related interactions\n",
    "season_cols = [col for col in available_columns if 'season' in col]\n",
    "if 'temp' in available_columns and season_cols:\n",
    "    print(\"Adding season  temperature interactions for columns:\", season_cols)\n",
    "    for season_col in season_cols:\n",
    "        X_train_interactions[f'{season_col}_temp'] = X_train_interactions[season_col] * X_train_interactions['temp']\n",
    "        X_val_interactions[f'{season_col}_temp'] = X_val_interactions[season_col] * X_val_interactions['temp']\n",
    "        X_test_interactions[f'{season_col}_temp'] = X_test_interactions[season_col] * X_test_interactions['temp']\n",
    "else:\n",
    "    print(\"Skipping season  temperature interactions (required columns not available)\")\n",
    "\n",
    "# 4. Create squared terms for important numerical features\n",
    "for col in ['temp', 'hum', 'windspeed']:\n",
    "    if col in available_columns:\n",
    "        print(f\"Adding squared term for {col}\")\n",
    "        X_train_interactions[f'{col}_squared'] = X_train_interactions[col] ** 2\n",
    "        X_val_interactions[f'{col}_squared'] = X_val_interactions[col] ** 2\n",
    "        X_test_interactions[f'{col}_squared'] = X_test_interactions[col] ** 2\n",
    "\n",
    "# Train a NEW model with refined features rather than using the previously tuned model\n",
    "# This is the key fix - don't use tuned_xgb which expects the original feature set\n",
    "print(f\"\\nTraining refined model with {X_train_interactions.shape[1]} features (added {X_train_interactions.shape[1] - X_train_processed.shape[1]} interaction features)\")\n",
    "\n",
    "# Create a new model with the same hyperparameters as the tuned model\n",
    "refined_model = XGBRegressor(**xgb_bayes_search.best_params_, random_state=42)\n",
    "refined_model.fit(X_train_interactions, y_train)\n",
    "\n",
    "# Make predictions with the refined model\n",
    "y_train_pred_refined = refined_model.predict(X_train_interactions)\n",
    "y_val_pred_refined = refined_model.predict(X_val_interactions)\n",
    "\n",
    "# Calculate metrics with the refined model\n",
    "refined_metrics = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred_refined),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred_refined),\n",
    "    'R2': r2_score(y_val, y_val_pred_refined)\n",
    "}\n",
    "\n",
    "print(\"\\nRefined Model Validation Metrics:\")\n",
    "for metric, value in refined_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Compare with base tuned model without interactions\n",
    "print(\"\\nImprovement over base tuned model:\")\n",
    "base_tuned_preds = tuned_xgb.predict(X_val_processed)\n",
    "base_metrics = {\n",
    "    'MSE': mean_squared_error(y_val, base_tuned_preds),\n",
    "    'MAE': mean_absolute_error(y_val, base_tuned_preds),\n",
    "    'R2': r2_score(y_val, base_tuned_preds)\n",
    "}\n",
    "\n",
    "for metric in refined_metrics:\n",
    "    improvement = (refined_metrics[metric] - base_metrics[metric]) / abs(base_metrics[metric]) * 100\n",
    "    better_worse = \"better\" if (metric == \"R2\" and improvement > 0) or (metric != \"R2\" and improvement < 0) else \"worse\"\n",
    "    print(f\"{metric}: {improvement:.2f}% {better_worse}\")\n",
    "\n",
    "# Plot feature importance of the refined model\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importance = pd.Series(refined_model.feature_importances_, index=X_train_interactions.columns).sort_values(ascending=False)\n",
    "top_features = feature_importance.head(15)\n",
    "top_features.plot.barh()\n",
    "plt.title('Top 15 Features in Refined Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation sets\n",
    "X_final_train = pd.concat([X_train_interactions, X_val_interactions])\n",
    "y_final_train = pd.concat([y_train, y_val])\n",
    "\n",
    "# Train final model on combined data\n",
    "final_model = tuned_xgb\n",
    "final_model.fit(X_final_train, y_final_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = final_model.predict(X_test_interactions)\n",
    "\n",
    "print(\"\\nFinal Model Test Performance:\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "# Feature importance of final model\n",
    "plt.figure(figsize=(12, 8))\n",
    "final_importance = pd.Series(final_model.feature_importances_, index=X_final_train.columns)\n",
    "final_importance.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Features in Final Model')\n",
    "plt.show()\n",
    "\n",
    "# Compare with validation performance\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"Validation R2:\", r2_score(y_val, y_val_pred_refined))\n",
    "print(\"Test R2:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional statistics for model predictions\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Create analysis for test predictions\n",
    "print(\"=\"*50)\n",
    "print(\"DETAILED MODEL PREDICTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate error statistics\n",
    "absolute_errors = np.abs(y_test - y_test_pred)\n",
    "percentage_errors = np.abs((y_test - y_test_pred) / y_test) * 100\n",
    "squared_errors = (y_test - y_test_pred) ** 2\n",
    "\n",
    "# Basic error statistics\n",
    "print(\"\\nError Statistics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {np.mean(absolute_errors):.2f}\")\n",
    "print(f\"Median Absolute Error: {np.median(absolute_errors):.2f}\")\n",
    "print(f\"90th Percentile of Absolute Error: {np.percentile(absolute_errors, 90):.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {np.mean(percentage_errors):.2f}%\")\n",
    "print(f\"Root Mean Square Error (RMSE): {np.sqrt(np.mean(squared_errors)):.2f}\")\n",
    "\n",
    "# Calculate prediction statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(f\"Mean of Actual Values: {np.mean(y_test):.2f}\")\n",
    "print(f\"Mean of Predicted Values: {np.mean(y_test_pred):.2f}\")\n",
    "print(f\"Standard Deviation of Actual Values: {np.std(y_test):.2f}\")\n",
    "print(f\"Standard Deviation of Predicted Values: {np.std(y_test_pred):.2f}\")\n",
    "print(f\"Min of Actual Values: {np.min(y_test):.2f}\")\n",
    "print(f\"Min of Predicted Values: {np.min(y_test_pred):.2f}\")\n",
    "print(f\"Max of Actual Values: {np.max(y_test):.2f}\")\n",
    "print(f\"Max of Predicted Values: {np.max(y_test_pred):.2f}\")\n",
    "\n",
    "# Calculate over/under prediction statistics\n",
    "over_predictions = y_test_pred > y_test\n",
    "under_predictions = y_test_pred < y_test\n",
    "exact_predictions = y_test_pred == y_test\n",
    "\n",
    "print(\"\\nOver/Under Prediction Analysis:\")\n",
    "print(f\"Over-predictions: {np.sum(over_predictions)} ({np.mean(over_predictions)*100:.1f}%)\")\n",
    "print(f\"Under-predictions: {np.sum(under_predictions)} ({np.mean(under_predictions)*100:.1f}%)\")\n",
    "print(f\"Exact predictions: {np.sum(exact_predictions)} ({np.mean(exact_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Calculate error distribution by actual value ranges\n",
    "def error_by_range(y_true, y_pred, bin_edges):\n",
    "    bin_indices = np.digitize(y_true, bin_edges) - 1\n",
    "    bin_count = len(bin_edges) - 1\n",
    "    bin_mae = np.zeros(bin_count)\n",
    "    bin_mape = np.zeros(bin_count)\n",
    "    bin_counts = np.zeros(bin_count)\n",
    "    \n",
    "    for i in range(bin_count):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_mae[i] = np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "            bin_mape[i] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "            bin_counts[i] = np.sum(mask)\n",
    "    \n",
    "    return bin_mae, bin_mape, bin_counts\n",
    "\n",
    "# Create bin edges based on the distribution of actual values\n",
    "bin_edges = np.percentile(y_test, [0, 25, 50, 75, 100])\n",
    "bin_mae, bin_mape, bin_counts = error_by_range(y_test, y_test_pred, bin_edges)\n",
    "\n",
    "print(\"\\nError Analysis by Actual Value Range:\")\n",
    "print(f\"{'Range':<20} {'Count':<10} {'MAE':<10} {'MAPE (%)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(bin_edges)-1):\n",
    "    print(f\"{bin_edges[i]:.0f}-{bin_edges[i+1]:.0f}:{' ':>10} {bin_counts[i]:<10.0f} {bin_mae[i]:<10.2f} {bin_mape[i]:<10.2f}\")\n",
    "\n",
    "# Prediction accuracy by quartile\n",
    "quartiles = np.percentile(y_test, [25, 50, 75])\n",
    "q1_mask = y_test <= quartiles[0]\n",
    "q2_mask = (y_test > quartiles[0]) & (y_test <= quartiles[1])\n",
    "q3_mask = (y_test > quartiles[1]) & (y_test <= quartiles[2])\n",
    "q4_mask = y_test > quartiles[2]\n",
    "\n",
    "print(\"\\nPrediction Performance by Quartile:\")\n",
    "print(f\"Q1 ({quartiles[0]:.2f}): R = {r2_score(y_test[q1_mask], y_test_pred[q1_mask]):.4f}, MAE = {mean_absolute_error(y_test[q1_mask], y_test_pred[q1_mask]):.2f}\")\n",
    "print(f\"Q2 ({quartiles[0]:.2f}-{quartiles[1]:.2f}): R = {r2_score(y_test[q2_mask], y_test_pred[q2_mask]):.4f}, MAE = {mean_absolute_error(y_test[q2_mask], y_test_pred[q2_mask]):.2f}\")\n",
    "print(f\"Q3 ({quartiles[1]:.2f}-{quartiles[2]:.2f}): R = {r2_score(y_test[q3_mask], y_test_pred[q3_mask]):.4f}, MAE = {mean_absolute_error(y_test[q3_mask], y_test_pred[q3_mask]):.2f}\")\n",
    "print(f\"Q4 (>{quartiles[2]:.2f}): R = {r2_score(y_test[q4_mask], y_test_pred[q4_mask]):.4f}, MAE = {mean_absolute_error(y_test[q4_mask], y_test_pred[q4_mask]):.2f}\")\n",
    "\n",
    "# Visualize error distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Histogram of errors\n",
    "plt.subplot(2, 2, 1)\n",
    "errors = y_test - y_test_pred\n",
    "sns.histplot(errors, kde=True, bins=30)\n",
    "plt.title('Distribution of Prediction Errors', fontsize=12)\n",
    "plt.xlabel('Error (Actual - Predicted)')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot of error vs actual value\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, errors, alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Error vs Actual Value', fontsize=12)\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Box plot of absolute errors by value quartile\n",
    "plt.subplot(2, 2, 3)\n",
    "quartile_names = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "quartile_errors = [absolute_errors[q1_mask], absolute_errors[q2_mask], \n",
    "                   absolute_errors[q3_mask], absolute_errors[q4_mask]]\n",
    "plt.boxplot(quartile_errors, labels=quartile_names)\n",
    "plt.title('Absolute Errors by Value Quartile', fontsize=12)\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Actual vs predicted with perfect prediction line\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "min_val = min(np.min(y_test), np.min(y_test_pred))\n",
    "max_val = max(np.max(y_test), np.max(y_test_pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.title('Actual vs Predicted Values', fontsize=12)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(checkpoint_dir, 'prediction_analysis.png'))\n",
    "plt.show()\n",
    "\n",
    "# Calculate prediction metrics for specific subgroups\n",
    "# For example, by season, hour of day, or weather condition\n",
    "print(\"\\nPerformance by Weather Condition:\")\n",
    "for weather in np.unique(X_test['weathersit']):\n",
    "    weather_mask = X_test['weathersit'] == weather\n",
    "    if np.sum(weather_mask) > 0:\n",
    "        mae = mean_absolute_error(y_test[weather_mask], y_test_pred[weather_mask])\n",
    "        r2 = r2_score(y_test[weather_mask], y_test_pred[weather_mask])\n",
    "        print(f\"Weather {weather}: Count = {np.sum(weather_mask)}, MAE = {mae:.2f}, R = {r2:.4f}\")\n",
    "\n",
    "# Calculate time-based prediction performance\n",
    "print(\"\\nPerformance by Hour Range:\")\n",
    "morning = (X_test['hr'] >= 6) & (X_test['hr'] < 12)\n",
    "afternoon = (X_test['hr'] >= 12) & (X_test['hr'] < 18)\n",
    "evening = (X_test['hr'] >= 18) & (X_test['hr'] < 22)\n",
    "night = (X_test['hr'] >= 22) | (X_test['hr'] < 6)\n",
    "\n",
    "print(f\"Morning (6-11): MAE = {mean_absolute_error(y_test[morning], y_test_pred[morning]):.2f}, R = {r2_score(y_test[morning], y_test_pred[morning]):.4f}\")\n",
    "print(f\"Afternoon (12-17): MAE = {mean_absolute_error(y_test[afternoon], y_test_pred[afternoon]):.2f}, R = {r2_score(y_test[afternoon], y_test_pred[afternoon]):.4f}\")\n",
    "print(f\"Evening (18-21): MAE = {mean_absolute_error(y_test[evening], y_test_pred[evening]):.2f}, R = {r2_score(y_test[evening], y_test_pred[evening]):.4f}\")\n",
    "print(f\"Night (22-5): MAE = {mean_absolute_error(y_test[night], y_test_pred[night]):.2f}, R = {r2_score(y_test[night], y_test_pred[night]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(checkpoint_dir, 'bike_rental_final_model.pkl')\n",
    "\n",
    "# Create a new model with the same hyperparameters as the tuned model but for our interaction features\n",
    "final_model = XGBRegressor(**xgb_bayes_search.best_params_, random_state=42)\n",
    "final_model.fit(X_final_train, y_final_train)  # Make sure to fit the model\n",
    "\n",
    "# Save the final model\n",
    "with open(final_model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# Save the feature scaler instead of the undefined preprocessor\n",
    "feature_scaler_path = os.path.join(checkpoint_dir, 'feature_scaler.pkl')\n",
    "with open(feature_scaler_path, 'wb') as f:\n",
    "    pickle.dump(feature_scaler, f)  # We defined feature_scaler earlier in the notebook\n",
    "\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "print(f\"Feature scaler saved to {feature_scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add accuracy metrics to the final model evaluation\n",
    "print(\"\\nACCURACY METRICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate R-squared (coefficient of determination)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print(f\"R Score: {r2:.4f}\")\n",
    "\n",
    "# Calculate explained variance score\n",
    "explained_var = explained_variance_score(y_test, y_test_pred)\n",
    "print(f\"Explained Variance: {explained_var:.4f}\")\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Calculate normalized RMSE (as percentage of mean value)\n",
    "normalized_rmse = (rmse / np.mean(y_test)) * 100\n",
    "print(f\"Normalized RMSE: {normalized_rmse:.2f}%\")\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error\n",
    "mape = np.mean(np.abs((y_test - y_test_pred) / y_test)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "# Calculate prediction accuracy within different error margins\n",
    "within_10pct = np.mean(np.abs((y_test - y_test_pred) / y_test) <= 0.10) * 100\n",
    "within_20pct = np.mean(np.abs((y_test - y_test_pred) / y_test) <= 0.20) * 100\n",
    "within_30pct = np.mean(np.abs((y_test - y_test_pred) / y_test) <= 0.30) * 100\n",
    "\n",
    "print(\"\\nPrediction Accuracy:\")\n",
    "print(f\"Predictions within 10% of actual value: {within_10pct:.1f}%\")\n",
    "print(f\"Predictions within 20% of actual value: {within_20pct:.1f}%\") \n",
    "print(f\"Predictions within 30% of actual value: {within_30pct:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
