{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github\n",
    "https://github.com/Shadowjumper3000/ML-fundamentals-2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike Rental Analysis\n",
    "This analysis explores patterns in bike rental data to understand key factors influencing rental behavior. The data is sourced from the UCI Machine Learning Repository and contains hourly rental data spanning two years. The analysis is structured as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Kaggle environment\n",
    "import os\n",
    "IN_KAGGLE = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    # Kaggle-specific paths\n",
    "    data_path = '/kaggle/input/bike-data/CapitalBikeSharing.csv'\n",
    "    checkpoint_dir = '/kaggle/working/models'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Create output directory for downloading models\n",
    "    os.makedirs('/kaggle/working/output', exist_ok=True)\n",
    "\n",
    "    print(\"Running in Kaggle environment.\")\n",
    "    print(\"Data will be loaded from:\", data_path)\n",
    "    print(\"Models will be saved to:\", checkpoint_dir)\n",
    "else:\n",
    "    # Local paths\n",
    "    data_path = '../data/hour.csv'\n",
    "    checkpoint_dir = '../models/checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"Running in local environment.\")\n",
    "    print(\"Data will be loaded from:\", data_path)\n",
    "    print(\"Models will be saved to:\", checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_theme()\n",
    "\n",
    "# Load the dataset\n",
    "hour_data = pd.read_csv(data_path)\n",
    "print(\"Data loaded from\" + data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Data Exploration\n",
    "Let's examine the basic structure and statistics of our dataset.\n",
    "AI was used to write the functions for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Exploration\n",
    "print(\"Dataset Shape:\", hour_data.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "print(hour_data.info())\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(hour_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Target Variable Analysis\n",
    "Analyzing the distribution of bike rentals (cnt) to understand the general rental patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Analysis (cnt)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(hour_data['cnt'], kde=True)\n",
    "plt.title('Distribution of Bike Rentals (cnt)')\n",
    "plt.xlabel('Number of Rentals')\n",
    "plt.ylabel('Frequency')\n",
    "print(\"\\nSkewness of cnt:\", hour_data['cnt'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Temporal Pattern Analysis\n",
    "Examining how rental patterns vary by hour and season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Hour analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='hr', y='cnt', data=hour_data)\n",
    "plt.title('Hourly Rental Pattern')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Rentals')\n",
    "plt.show()\n",
    "\n",
    "# Rentals per year\n",
    "# Calculate total rentals per year - safer approach\n",
    "yearly_rentals = hour_data.groupby('yr')['cnt'].sum().reset_index()\n",
    "yearly_rentals['year'] = yearly_rentals['yr'].map({0: '2011', 1: '2012'})\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.barplot(x='year', y='cnt', data=yearly_rentals, \n",
    "                color=sns.color_palette(\"Blues_d\")[3])\n",
    "plt.title('Total Rentals Per Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Rentals')\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):,}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "plt.show()\n",
    "\n",
    "# Rentals per month\n",
    "# Calculate total rentals per month - more robust approach\n",
    "monthly_rentals = hour_data.groupby('mnth')['cnt'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x='mnth', y='cnt', data=monthly_rentals, \n",
    "                color=sns.color_palette(\"Blues_d\")[3])\n",
    "plt.title('Total Rentals Per Month', pad=15)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Rentals')\n",
    "plt.xticks(ticks=range(12), \n",
    "          labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "          rotation=45)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):,}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a cross-tabulation between season and month\n",
    "season_month_crosstab = pd.crosstab(\n",
    "    hour_data['season'],\n",
    "    hour_data['mnth']\n",
    ")\n",
    "\n",
    "# Perform chi-square test for independence\n",
    "chi2, p, dof, expected = chi2_contingency(season_month_crosstab)\n",
    "\n",
    "print(f\"Chi2: {chi2:.2f}\")\n",
    "print(f\"p-value: {p:.10f}\")\n",
    "print(f\"DOF: {dof}\")\n",
    "print(f\"Hypothesis: {'Rejected' if p < 0.05 else 'Not rejected'}\")\n",
    "\n",
    "# Compare correlation with target variable\n",
    "print(\"\\nCorrelation with target variable:\")\n",
    "print(f\"Season-cnt correlation: {hour_data['season'].corr(hour_data['cnt']):.4f}\")\n",
    "print(f\"Month-cnt correlation: {hour_data['mnth'].corr(hour_data['cnt']):.4f}\")\n",
    "\n",
    "# Check distribution of values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='season', data=hour_data)\n",
    "plt.title('Season Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='mnth', data=hour_data)\n",
    "plt.title('Month Distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the 'Seasons' and 'mnth's are correlated, we will drop the 'mnth' column as this is less correlated with 'cnt' than 'seasons'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the mnth column as it's highly correlated with season\n",
    "hour_data = hour_data.drop(columns=['mnth'])\n",
    "print(\"Remaining columns:\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Holiday and Working Day Analysis\n",
    "Investigating how holidays and working days affect rental patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Holiday analysis at hourly level\n",
    "holiday_labels = {0: 'Non-Holiday', 1: 'Holiday'}\n",
    "sns.boxplot(x='holiday', y='cnt', data=hour_data, ax=ax1)\n",
    "ax1.set_title('Hourly Rentals: Holiday vs Non-Holiday')\n",
    "ax1.set_xticklabels([holiday_labels[i] for i in [0, 1]])\n",
    "ax1.set_xlabel('Day Type')\n",
    "ax1.set_ylabel('Hourly Rentals')\n",
    "\n",
    "# Working day analysis at hourly level\n",
    "workingday_labels = {0: 'Non-Working Day', 1: 'Working Day'}\n",
    "sns.boxplot(x='workingday', y='cnt', data=hour_data, ax=ax2)\n",
    "ax2.set_title('Hourly Rentals: Working vs Non-Working Days')\n",
    "ax2.set_xticklabels([workingday_labels[i] for i in [0, 1]])\n",
    "ax2.set_xlabel('Day Type')\n",
    "ax2.set_ylabel('Hourly Rentals')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create day of week visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate average rentals by day of week\n",
    "weekday_rentals = hour_data.groupby('weekday')['cnt'].mean().reset_index()\n",
    "\n",
    "# Map weekday numbers to names for better readability\n",
    "weekday_names = {\n",
    "    0: 'Sunday',\n",
    "    1: 'Monday',\n",
    "    2: 'Tuesday',\n",
    "    3: 'Wednesday',\n",
    "    4: 'Thursday',\n",
    "    5: 'Friday',\n",
    "    6: 'Saturday'\n",
    "}\n",
    "weekday_rentals['day_name'] = weekday_rentals['weekday'].map(weekday_names)\n",
    "\n",
    "# Create bar plot with improved styling\n",
    "ax = sns.barplot(x='day_name', y='cnt', data=weekday_rentals, \n",
    "                order=[weekday_names[i] for i in range(7)],\n",
    "                palette='Blues_d')\n",
    "plt.title('Average Hourly Bike Rentals by Day of Week', fontsize=16, pad=20)\n",
    "plt.xlabel('Day of Week', fontsize=14)\n",
    "plt.ylabel('Average Hourly Rentals', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.1f}', \n",
    "               (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "               ha='center', va='bottom',\n",
    "               xytext=(0, 5),\n",
    "               textcoords='offset points')\n",
    "\n",
    "# Add weekday vs weekend comparison\n",
    "weekday_mask = hour_data['weekday'].isin([1, 2, 3, 4, 5])\n",
    "weekend_mask = hour_data['weekday'].isin([0, 6])\n",
    "weekday_avg = hour_data[weekday_mask]['cnt'].mean()\n",
    "weekend_avg = hour_data[weekend_mask]['cnt'].mean()\n",
    "diff_pct = ((weekday_avg - weekend_avg) / weekend_avg) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weekday effect on bike rentals (cnt) with hourly patterns by day\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create hourly patterns by day of week\n",
    "hourly_weekday_data = hour_data.pivot_table(\n",
    "    values='cnt', \n",
    "    index='hr', \n",
    "    columns='weekday', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Map weekday numbers to names for better readability\n",
    "hourly_weekday_data.columns = [weekday_names[day] for day in hourly_weekday_data.columns]\n",
    "\n",
    "# Plot hourly patterns by day of week\n",
    "for day in hourly_weekday_data.columns:\n",
    "    plt.plot(hourly_weekday_data.index, hourly_weekday_data[day], \n",
    "             label=day, marker='o', markersize=5, alpha=0.7)\n",
    "\n",
    "# Add highlights for peak hours\n",
    "plt.axvspan(7, 9, color='lightblue', alpha=0.3, label='Morning Peak')\n",
    "plt.axvspan(16, 19, color='lightyellow', alpha=0.3, label='Evening Peak')\n",
    "\n",
    "plt.title('Hourly Rental Patterns by Day of Week', fontsize=16)\n",
    "plt.xlabel('Hour of Day', fontsize=12)\n",
    "plt.ylabel('Average Rentals', fontsize=12)\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Day of Week', loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "           fancybox=True, shadow=True, ncol=4)\n",
    "\n",
    "# Calculate and display correlation between weekday and cnt\n",
    "weekday_cnt_corr = hour_data['weekday'].corr(hour_data['cnt'])\n",
    "plt.annotate(f'Correlation between weekday and cnt: {weekday_cnt_corr:.3f}',\n",
    "            xy=(0.5, 0.97), xycoords='axes fraction',\n",
    "            ha='center', fontsize=12,\n",
    "            bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the spread of rentals on weekday vs weekend, we will drop the 'weekday' column as it is less correlated with 'cnt' than 'workingday'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the weekday column as it's less correlated with 'cnt' than 'workingday'\n",
    "hour_data = hour_data.drop(columns=['weekday'])\n",
    "print(f\"'weekday' column dropped. Remaining columns: {hour_data.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Weather Impact Analysis\n",
    "Analyzing how different weather conditions affect rental behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weather situation mapping\n",
    "weather_labels = {\n",
    "    1: 'Clear/Partly Cloudy',\n",
    "    2: 'Mist/Cloudy',\n",
    "    3: 'Light Rain/Snow',\n",
    "    4: 'Heavy Rain/Snow'\n",
    "}\n",
    "\n",
    "# Temperature vs Count with improved styling\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='temp', y='cnt', data=hour_data, alpha=0.5)\n",
    "plt.title('Impact of Temperature on Bike Rentals', pad=15)\n",
    "plt.xlabel('Temperature (Normalized 0-1 scale)')\n",
    "plt.ylabel('Number of Hourly Rentals')\n",
    "\n",
    "# Add trend line\n",
    "sns.regplot(x='temp', y='cnt', data=hour_data, scatter=False, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Weather Situation vs Count with descriptive labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='weathersit', y='cnt', data=hour_data)\n",
    "plt.title('Impact of Weather Conditions on Bike Rentals', pad=15)\n",
    "plt.xlabel('Weather Condition')\n",
    "plt.ylabel('Number of Hourly Rentals')\n",
    "\n",
    "# Update x-axis labels with weather descriptions\n",
    "plt.xticks(range(len(weather_labels)), \n",
    "          [weather_labels[i] for i in range(1, 5)], \n",
    "          rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Correlation Analysis\n",
    "Examining relationships between numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = {\n",
    "    'cnt': 'Total Rentals',\n",
    "    'temp': 'Temperature',\n",
    "    'atemp': 'Air Temp',\n",
    "    'hum': 'Humidity',\n",
    "    'windspeed': 'Wind Speed',\n",
    "    'hr': 'Hour',\n",
    "    'season': 'Season'\n",
    "}\n",
    "\n",
    "correlation_features = ['cnt', 'temp', 'atemp', 'hum', 'windspeed', 'hr', 'season']\n",
    "correlation_matrix = hour_data[correlation_features].corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            xticklabels=[feature_labels[col] for col in correlation_features],\n",
    "            yticklabels=[feature_labels[col] for col in correlation_features])\n",
    "plt.title('Correlation Matrix of Numerical Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rental distribution statistics\n",
    "print(\"1. Distribution Statistics:\")\n",
    "print(f\"Mean rentals: {hour_data['cnt'].mean():.2f}\")\n",
    "print(f\"Median rentals: {hour_data['cnt'].median():.2f}\")\n",
    "print(f\"Skewness: {hour_data['cnt'].skew():.2f}\")\n",
    "\n",
    "# Calculate peak hours statistics\n",
    "hourly_avg = hour_data.groupby('hr')['cnt'].mean()\n",
    "peak_hours = hourly_avg.nlargest(3)\n",
    "print(\"\\n2. Peak Hours:\")\n",
    "print(peak_hours)\n",
    "\n",
    "# Calculate weather correlations\n",
    "print(\"\\n3. Weather Correlations:\")\n",
    "print(f\"Temperature correlation: {hour_data['cnt'].corr(hour_data['temp']):.2f}\")\n",
    "print(f\"Humidity correlation: {hour_data['cnt'].corr(hour_data['hum']):.2f}\")\n",
    "print(f\"Wind speed correlation: {hour_data['cnt'].corr(hour_data['windspeed']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop the 'instant' and 'dteday' columns as they are not needed for our analysis. The 'instant' column is an index, and 'dteday' is a date column that we will not use in our analysis.\n",
    "'casual' and 'registered' columns are also dropped as they are only gathered after bike rental. We will only use the 'cnt' column as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index-like or redundant columns\n",
    "columns_to_drop = ['instant', 'dteday', 'casual', 'registered']\n",
    "\n",
    "# Drop columns and create clean dataset\n",
    "hour_data_clean = hour_data.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"\\nColumns dropped:\", columns_to_drop)\n",
    "print(\"Remaining columns:\", hour_data_clean.columns.tolist())\n",
    "\n",
    "# Update our working dataset\n",
    "hour_data = hour_data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between temp and atemp\n",
    "temp_correlation = hour_data['temp'].corr(hour_data['atemp'])\n",
    "\n",
    "# Create visualization to show relationship\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=hour_data, x='temp', y='atemp', alpha=0.5)\n",
    "plt.title(f'Temperature vs Apparent Temperature\\nCorrelation: {temp_correlation:.3f}')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Apparent Temperature (Feels Like)')\n",
    "\n",
    "# Print analysis\n",
    "print(f\"Correlation between temp and atemp: {temp_correlation:.3f}\")\n",
    "\n",
    "# Check their individual correlations with the target variable\n",
    "temp_target_corr = hour_data['temp'].corr(hour_data['cnt'])\n",
    "atemp_target_corr = hour_data['atemp'].corr(hour_data['cnt'])\n",
    "\n",
    "print(\"\\nCorrelations with rental count (cnt):\")\n",
    "print(f\"Temperature: {temp_target_corr:.3f}\")\n",
    "print(f\"Apparent Temperature: {atemp_target_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will dorp the 'temp' column as it is less correlated with 'cnt' than 'atemp'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = hour_data.drop(columns=['atemp'])\n",
    "print(\"Columns remaining after dropping 'atemp':\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any remaining problematic columns\n",
    "df_clean = hour_data\n",
    "del hour_data\n",
    "\n",
    "print(\"Final columns in cleaned dataset:\", df_clean.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our data into three sets:\n",
    "- Training set (60%): Used to train the model\n",
    "- Validation set (20%): Used to tune hyperparameters and evaluate model during training\n",
    "- Test set (20%): Used for final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure the date column is properly formatted\n",
    "import pandas as pd\n",
    "\n",
    "# Add a helper column if not already present to sort chronologically\n",
    "df_clean['datetime'] = pd.to_datetime(df_clean['dteday']) + pd.to_timedelta(df_clean['hr'], unit='h')\n",
    "\n",
    "# Sort by datetime to ensure chronological order\n",
    "df_clean = df_clean.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "# Create the splits based on time\n",
    "total_rows = len(df_clean)\n",
    "train_end = int(total_rows * 0.6)\n",
    "val_end = int(total_rows * 0.8)\n",
    "\n",
    "# Create train, validation, and test datasets\n",
    "X = df_clean.drop('cnt', axis=1)\n",
    "y = df_clean['cnt']\n",
    "\n",
    "X_train = X.iloc[:train_end]\n",
    "y_train = y.iloc[:train_end]\n",
    "\n",
    "X_val = X.iloc[train_end:val_end]\n",
    "y_val = y.iloc[train_end:val_end]\n",
    "\n",
    "X_test = X.iloc[val_end:]\n",
    "y_test = y.iloc[val_end:]\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} ({len(X_train)/len(X):.1%})\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} ({len(X_val)/len(X):.1%})\")\n",
    "print(f\"Test set size: {X_test.shape[0]} ({len(X_test)/len(X):.1%})\")\n",
    "\n",
    "# Visualize the time periods for each split\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_clean['datetime'], df_clean['cnt'], color='lightgray', alpha=0.5, label='All Data')\n",
    "plt.plot(X_train['datetime'], y_train, color='blue', alpha=0.7, label='Training Set')\n",
    "plt.plot(X_val['datetime'], y_val, color='green', alpha=0.7, label='Validation Set')\n",
    "plt.plot(X_test['datetime'], y_test, color='red', alpha=0.7, label='Test Set')\n",
    "plt.title('Rolling Anchor Time Series Split')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Bike Rentals')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart showing the data split proportions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate percentages\n",
    "total = len(X)\n",
    "train_pct = len(X_train) / total * 100\n",
    "val_pct = len(X_val) / total * 100 \n",
    "test_pct = len(X_test) / total * 100\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.pie([train_pct, val_pct, test_pct], \n",
    "        labels=['Training Set', 'Validation Set', 'Test Set'],\n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#2ecc71', '#3498db', '#e74c3c'])\n",
    "plt.title('Dataset Split Distribution')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create cross-tabulation\n",
    "workingday_weekday_crosstab = pd.crosstab(hour_data['workingday'], hour_data['weekday'])\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(workingday_weekday_crosstab)\n",
    "\n",
    "print(f\"Chi2: {chi2:.2f}\")\n",
    "print(f\"p-value: {p:.10f}\")\n",
    "print(f\"DOF: {dof}\")\n",
    "print(f\"Hypothesis: {'Rejected' if p < 0.05 else 'Not rejected'}\")\n",
    "\n",
    "# Working vs non-working day average comparison\n",
    "working_avg = hour_data[hour_data['workingday'] == 1]['cnt'].mean()\n",
    "non_working_avg = hour_data[hour_data['workingday'] == 0]['cnt'].mean()\n",
    "diff_pct = ((working_avg - non_working_avg) / non_working_avg) * 100\n",
    "print(f\"Working day rentals: {working_avg:.2f}\")\n",
    "print(f\"Non-working day rentals: {non_working_avg:.2f}\")\n",
    "print(f\"Difference: {diff_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to Workingday and Weekday being somewhat correlated, I will be one-hot encoding the Weekday variable to avoid multicollinearity issues. This will allow us to analyze the impact of each weekday on bike rentals while keeping the Workingday variable intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simplified binary day_type feature (1 = workday, 0 = weekend/holiday)\n",
    "def categorize_binary_day_type(row):\n",
    "    if row['holiday'] == 1 or (row['workingday'] == 0 and row['holiday'] == 0):\n",
    "        return 0  # Holiday or weekend\n",
    "    else:\n",
    "        return 1  # Regular working weekday\n",
    "\n",
    "# Apply this to the hour_data dataframe\n",
    "hour_data['is_workday'] = hour_data.apply(categorize_binary_day_type, axis=1)\n",
    "\n",
    "# Display the distribution of binary day types\n",
    "binary_day_counts = hour_data['is_workday'].value_counts()\n",
    "print(\"Distribution of binary day types:\")\n",
    "print(f\"Non-workdays (0): {binary_day_counts.get(0, 0)}\")\n",
    "print(f\"Workdays (1): {binary_day_counts.get(1, 0)}\")\n",
    "\n",
    "# Drop the now-redundant columns\n",
    "hour_data = hour_data.drop(columns=['holiday', 'workingday', 'weekday'])\n",
    "print(\"Columns after dropping redundant day features:\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be one-hot encoding the weather variable as well to capture the impact of different weather conditions on bike rentals. This will allow us to analyze the effect of each weather condition while keeping the original variable intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the weathersit column using pandas get_dummies\n",
    "weather_encoded = pd.get_dummies(hour_data['weathersit'], prefix='weathersit', drop_first=False)\n",
    "\n",
    "# Display the first few rows of the encoded data\n",
    "print(\"Weather condition encoded features:\")\n",
    "print(weather_encoded.head())\n",
    "\n",
    "# Add the encoded columns to the original dataframe\n",
    "hour_data = pd.concat([hour_data, weather_encoded], axis=1)\n",
    "\n",
    "# Drop the original weathersit column\n",
    "hour_data = hour_data.drop(columns=['weathersit'])\n",
    "\n",
    "print(\"\\nColumns after encoding weathersit:\", hour_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will categorize 'atemp', 'hum' and 'windspeed' as numeric features.\n",
    "We will categorize 'season', 'mnth', 'holiday', 'workingday' and 'weather' as categorical features.\n",
    "We will categorize 'hr' and 'weekday' as cyclic features to capture the cyclical nature of time.\n",
    "\n",
    "For the cyclical features we will use sine and cosine transformations to represent the cyclical nature of time. This allows us to capture the periodicity of these features in a way that is meaningful for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = lr_model.predict(X_train_processed)\n",
    "y_val_pred = lr_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "    'R2': r2_score(y_val, y_val_pred)\n",
    "}\n",
    "\n",
    "print(\"Validation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals = y_val - y_val_pred\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.show()\n",
    "\n",
    "# Analyze bias and variance\n",
    "print(\"\\nBias-Variance Analysis:\")\n",
    "print(f\"Training R2: {r2_score(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Validation R2: {r2_score(y_val, y_val_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and train the model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train_processed)\n",
    "y_val_pred_rf = rf_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_rf = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred_rf),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred_rf),\n",
    "    'R2': r2_score(y_val, y_val_pred_rf)\n",
    "}\n",
    "\n",
    "print(\"Random Forest Validation Metrics:\")\n",
    "for metric, value in metrics_rf.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Feature importance plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=feature_names)\n",
    "feature_importance.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.show()\n",
    "\n",
    "# Compare with baseline\n",
    "print(\"\\nComparison with Linear Regression:\")\n",
    "for metric in metrics.keys():\n",
    "    improvement = (metrics_rf[metric] - metrics[metric]) / abs(metrics[metric]) * 100\n",
    "    print(f\"{metric} improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Random Forest model\n",
    "import pickle\n",
    "\n",
    "# Save the current model if tuned version isn't available yet\n",
    "rf_model_path = os.path.join(checkpoint_dir, 'rf_model.pkl')\n",
    "with open(rf_model_path, 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(f\"Random Forest model saved to {rf_model_path}\")\n",
    "\n",
    "# We'll save the tuned version later after hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize and train the model with verbose logging\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model = XGBRegressor(\n",
    "    random_state=42, \n",
    "    verbose=1,\n",
    "    eval_metric=['rmse', 'mae'],\n",
    "    early_stopping_rounds=10  # Move this parameter here\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_train_processed, \n",
    "    y_train, \n",
    "    eval_set=[(X_train_processed, y_train), (X_val_processed, y_val)],\n",
    "    verbose=True  # Keep this to see progress during training\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"XGBoost training completed in {training_time:.2f} seconds\\n\")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train_processed)\n",
    "y_val_pred_xgb = xgb_model.predict(X_val_processed)\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "train_metrics = {\n",
    "    'MSE': mean_squared_error(y_train, y_train_pred_xgb),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred_xgb)),\n",
    "    'MAE': mean_absolute_error(y_train, y_train_pred_xgb),\n",
    "    'R2': r2_score(y_train, y_train_pred_xgb),\n",
    "    'Explained Variance': explained_variance_score(y_train, y_train_pred_xgb)\n",
    "}\n",
    "\n",
    "val_metrics = {\n",
    "    'MSE': mean_squared_error(y_val, y_val_pred_xgb),\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred_xgb)),\n",
    "    'MAE': mean_absolute_error(y_val, y_val_pred_xgb),\n",
    "    'R2': r2_score(y_val, y_val_pred_xgb),\n",
    "    'Explained Variance': explained_variance_score(y_val, y_val_pred_xgb)\n",
    "}\n",
    "\n",
    "# Print metrics in a formatted table\n",
    "print(\"XGBoost Performance Metrics:\")\n",
    "print(f\"{'Metric':<20} {'Training':<15} {'Validation':<15}\")\n",
    "print(\"-\" * 50)\n",
    "for metric in train_metrics.keys():\n",
    "    print(f\"{metric:<20} {train_metrics[metric]:<15.4f} {val_metrics[metric]:<15.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance = pd.Series(xgb_model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
    "ax = feature_importance.head(15).plot(kind='barh', \n",
    "                                     color=sns.color_palette(\"viridis\", len(feature_importance.head(15))))\n",
    "plt.title('Top 15 Most Important Features in XGBoost Model', fontsize=14)\n",
    "plt.xlabel('Importance Score', fontsize=12)\n",
    "plt.ylabel('Features', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add value annotations to the bars\n",
    "for i, v in enumerate(feature_importance.head(15)):\n",
    "    ax.text(v + 0.01, i, f'{v:.3f}', va='center')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "residuals_xgb = y_val - y_val_pred_xgb\n",
    "sns.histplot(residuals_xgb, kde=True, bins=30, color='steelblue')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribution of XGBoost Residuals', fontsize=14)\n",
    "plt.xlabel('Residual Value', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Create a scatter plot of actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_val_pred_xgb, alpha=0.5, color='steelblue')\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "plt.title('XGBoost: Actual vs Predicted Values', fontsize=14)\n",
    "plt.xlabel('Actual', fontsize=12)\n",
    "plt.ylabel('Predicted', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Compare with previous models\n",
    "print(\"\\nComparison with Previous Models:\")\n",
    "for metric in ['MSE', 'MAE', 'R2']:\n",
    "    improvement_rf = (val_metrics[metric] - metrics_rf[metric]) / abs(metrics_rf[metric]) * 100\n",
    "    improvement_lr = (val_metrics[metric] - metrics[metric]) / abs(metrics[metric]) * 100\n",
    "    \n",
    "    # Display with colored text for better visualization\n",
    "    rf_status = \"✓ Better\" if ((metric == 'R2' and improvement_rf > 0) or \n",
    "                               (metric != 'R2' and improvement_rf < 0)) else \"✗ Worse\"\n",
    "    lr_status = \"✓ Better\" if ((metric == 'R2' and improvement_lr > 0) or \n",
    "                              (metric != 'R2' and improvement_lr < 0)) else \"✗ Worse\"\n",
    "    \n",
    "    print(f\"{metric} compared to Random Forest: {improvement_rf:.2f}% ({rf_status})\")\n",
    "    print(f\"{metric} compared to Linear Regression: {improvement_lr:.2f}% ({lr_status})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('hyperparameter_tuning')\n",
    "\n",
    "# Define parameter distributions with more detailed logging\n",
    "logger.info(\"Starting hyperparameter tuning process\")\n",
    "print(\"Defining parameter search spaces...\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(5, 30),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': [0.5, 0.7, 1.0, 'sqrt', 'log2', None]\n",
    "}\n",
    "logger.info(f\"Random Forest parameters to tune: {', '.join(rf_param_grid.keys())}\")\n",
    "print(\"Random Forest parameters to tune:\", \", \".join(rf_param_grid.keys()))\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'learning_rate': uniform(0.01, 0.3),\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "    'gamma': uniform(0, 1),\n",
    "    'min_child_weight': randint(1, 10)\n",
    "}\n",
    "logger.info(f\"XGBoost parameters to tune: {', '.join(xgb_param_grid.keys())}\")\n",
    "print(\"XGBoost parameters to tune:\", \", \".join(xgb_param_grid.keys()))\n",
    "\n",
    "# Custom callback for logging during random search\n",
    "class LoggingCallback:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.iteration = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def __call__(self, model, params):\n",
    "        self.iteration += 1\n",
    "        elapsed = time.time() - self.start_time\n",
    "        logger.info(f\"{self.model_name} - Iteration {self.iteration} completed in {elapsed:.2f}s\")\n",
    "        logger.info(f\"Current parameters: {params}\")\n",
    "        \n",
    "        # Every 10 iterations, print a summary\n",
    "        if self.iteration % 10 == 0:\n",
    "            print(f\"{self.model_name} - Completed {self.iteration} iterations in {elapsed:.2f}s\")\n",
    "\n",
    "# Random Forest Tuning with improved logging\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting Random Forest hyperparameter tuning...\")\n",
    "logger.info(\"Starting Random Forest hyperparameter tuning\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create callback for RF model\n",
    "rf_callback = LoggingCallback(\"Random Forest\")\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting RF RandomizedSearchCV with {rf_random_search.n_iter} iterations\")\n",
    "print(f\"Running {rf_random_search.n_iter} iterations of Random Forest hyperparameter search...\")\n",
    "\n",
    "try:\n",
    "    # Fit the model - we'll wrap this in a try-except to provide graceful error handling\n",
    "    rf_random_search.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Log results from each iteration for analysis\n",
    "    rf_results = pd.DataFrame(rf_random_search.cv_results_)\n",
    "    \n",
    "    # Log the top 5 best performing parameter combinations\n",
    "    logger.info(\"Top 5 Random Forest parameter combinations:\")\n",
    "    top_results = rf_results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "    for i, row in top_results.iterrows():\n",
    "        logger.info(f\"Rank {i+1}: Score = {-row['mean_test_score']:.4f}, Params = {row['params']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"RF tuning failed with error: {str(e)}\")\n",
    "    print(f\"Error during RF tuning: {str(e)}\")\n",
    "\n",
    "rf_tuning_time = time.time() - start_time\n",
    "logger.info(f\"Random Forest tuning completed in {rf_tuning_time:.2f} seconds\")\n",
    "print(f\"\\nRandom Forest tuning completed in {rf_tuning_time:.2f} seconds\")\n",
    "print(f\"Best MSE: {-rf_random_search.best_score_:.4f}\")\n",
    "print(\"Best Random Forest Parameters:\")\n",
    "for param, value in rf_random_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "    logger.info(f\"Best {param}: {value}\")\n",
    "\n",
    "# Create a bar plot of top parameter importance\n",
    "rf_results = pd.DataFrame(rf_random_search.cv_results_)\n",
    "rf_results = rf_results.sort_values('mean_test_score', ascending=False)\n",
    "logger.info(f\"Sorted {len(rf_results)} results by performance\")\n",
    "\n",
    "# XGBoost Tuning with improved logging\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting XGBoost hyperparameter tuning...\")\n",
    "logger.info(\"Starting XGBoost hyperparameter tuning\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create callback for XGB model\n",
    "xgb_callback = LoggingCallback(\"XGBoost\")\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    XGBRegressor(random_state=42),\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting XGB RandomizedSearchCV with {xgb_random_search.n_iter} iterations\")\n",
    "print(f\"Running {xgb_random_search.n_iter} iterations of XGBoost hyperparameter search...\")\n",
    "\n",
    "try:\n",
    "    # Fit the model\n",
    "    xgb_random_search.fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Log results from each iteration for analysis\n",
    "    xgb_results = pd.DataFrame(xgb_random_search.cv_results_)\n",
    "    \n",
    "    # Log the top 5 best performing parameter combinations\n",
    "    logger.info(\"Top 5 XGBoost parameter combinations:\")\n",
    "    top_results = xgb_results.sort_values('mean_test_score', ascending=False).head(5)\n",
    "    for i, row in top_results.iterrows():\n",
    "        logger.info(f\"Rank {i+1}: Score = {-row['mean_test_score']:.4f}, Params = {row['params']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"XGB tuning failed with error: {str(e)}\")\n",
    "    print(f\"Error during XGB tuning: {str(e)}\")\n",
    "\n",
    "xgb_tuning_time = time.time() - start_time\n",
    "logger.info(f\"XGBoost tuning completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"\\nXGBoost tuning completed in {xgb_tuning_time:.2f} seconds\")\n",
    "print(f\"Best MSE: {-xgb_random_search.best_score_:.4f}\")\n",
    "print(\"Best XGBoost Parameters:\")\n",
    "for param, value in xgb_random_search.best_params_.items():\n",
    "    print(f\"    {param}: {value}\")\n",
    "    logger.info(f\"Best {param}: {value}\")\n",
    "\n",
    "# Get the best models\n",
    "tuned_rf = rf_random_search.best_estimator_\n",
    "tuned_xgb = xgb_random_search.best_estimator_\n",
    "\n",
    "# Save detailed tuning results to CSV for later analysis\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "rf_results.to_csv(f\"{checkpoint_dir}/rf_tuning_results_{timestamp}.csv\", index=False)\n",
    "xgb_results.to_csv(f\"{checkpoint_dir}/xgb_tuning_results_{timestamp}.csv\", index=False)\n",
    "logger.info(f\"Tuning results saved to CSV files in {checkpoint_dir}\")\n",
    "\n",
    "# Create a function for detailed model evaluation\n",
    "def evaluate_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    logger.info(f\"Evaluating {model_name} model\")\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    train_metrics = {\n",
    "        'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'R2': r2_score(y_train, y_train_pred),\n",
    "        'Explained Variance': explained_variance_score(y_train, y_train_pred)\n",
    "    }\n",
    "    \n",
    "    val_metrics = {\n",
    "        'MSE': mean_squared_error(y_val, y_val_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "        'R2': r2_score(y_val, y_val_pred),\n",
    "        'Explained Variance': explained_variance_score(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    # Log all metrics\n",
    "    logger.info(f\"{model_name} - Train metrics: {train_metrics}\")\n",
    "    logger.info(f\"{model_name} - Validation metrics: {val_metrics}\")\n",
    "    \n",
    "    # Print metrics in a formatted table\n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    print(f\"{'Metric':<20} {'Training':<15} {'Validation':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric in train_metrics.keys():\n",
    "        print(f\"{metric:<20} {train_metrics[metric]:<15.4f} {val_metrics[metric]:<15.4f}\")\n",
    "    \n",
    "    eval_time = time.time() - eval_start_time\n",
    "    logger.info(f\"Model evaluation completed in {eval_time:.2f} seconds\")\n",
    "    \n",
    "    # Return metrics for further comparison\n",
    "    return train_metrics, val_metrics, y_val_pred\n",
    "\n",
    "# Evaluate the tuned models\n",
    "print(\"\\nEvaluating tuned models...\")\n",
    "logger.info(\"Starting evaluation of tuned models\")\n",
    "rf_train_metrics, rf_val_metrics, y_val_pred_tuned_rf = evaluate_model(\n",
    "    tuned_rf, X_train_processed, X_val_processed, y_train, y_val, \"Tuned Random Forest\")\n",
    "\n",
    "xgb_train_metrics, xgb_val_metrics, y_val_pred_tuned_xgb = evaluate_model(\n",
    "    tuned_xgb, X_train_processed, X_val_processed, y_train, y_val, \"Tuned XGBoost\")\n",
    "\n",
    "# Compare models and log differences\n",
    "for metric in ['MSE', 'MAE', 'R2']:\n",
    "    diff = ((xgb_val_metrics[metric] - rf_val_metrics[metric]) / abs(rf_val_metrics[metric])) * 100\n",
    "    logger.info(f\"XGBoost vs RF {metric} difference: {diff:.2f}%\")\n",
    "\n",
    "# Compare the tuned models visually\n",
    "metrics_to_compare = ['MSE', 'MAE', 'R2']\n",
    "plt.figure(figsize=(15, 6))\n",
    "logger.info(\"Creating comparison visualizations\")\n",
    "\n",
    "# Create bar chart to compare models\n",
    "x = np.arange(len(metrics_to_compare))\n",
    "width = 0.35\n",
    "\n",
    "# Plot validation metrics\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(x - width/2, [rf_val_metrics[m] for m in metrics_to_compare], width, label='Random Forest')\n",
    "plt.bar(x + width/2, [xgb_val_metrics[m] for m in metrics_to_compare], width, label='XGBoost')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Validation Metrics Comparison')\n",
    "plt.xticks(x, metrics_to_compare)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot residuals distribution for both models\n",
    "plt.subplot(1, 2, 2)\n",
    "rf_residuals = y_val - y_val_pred_tuned_rf\n",
    "xgb_residuals = y_val - y_val_pred_tuned_xgb\n",
    "sns.kdeplot(rf_residuals, label='Random Forest', fill=True, alpha=0.3)\n",
    "sns.kdeplot(xgb_residuals, label='XGBoost', fill=True, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='--')\n",
    "plt.title('Residuals Distribution')\n",
    "plt.xlabel('Residual Value')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.grid(linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{checkpoint_dir}/model_comparison_{timestamp}.png\")\n",
    "logger.info(f\"Comparison visualization saved to {checkpoint_dir}/model_comparison_{timestamp}.png\")\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"Hyperparameter tuning and evaluation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best XGBoost model\n",
    "import pickle\n",
    "\n",
    "# Save the tuned XGBoost model\n",
    "xgb_model_path = os.path.join(checkpoint_dir, 'xgb_tuned_model.pkl')\n",
    "with open(xgb_model_path, 'wb') as f:\n",
    "    pickle.dump(tuned_xgb, f)\n",
    "\n",
    "print(f\"Tuned XGBoost model saved to {xgb_model_path}\")\n",
    "\n",
    "# Function to load the model if needed\n",
    "def load_xgb_model(model_path=None):\n",
    "    if model_path is None:\n",
    "        model_path = os.path.join(checkpoint_dir, 'xgb_tuned_model.pkl')\n",
    "    with open(model_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evalutaion/Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL REFINEMENT AND FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Log feature engineering process\n",
    "print(\"Adding interaction terms based on EDA insights...\")\n",
    "X_train_interactions = X_train_processed.copy()\n",
    "X_val_interactions = X_val_processed.copy()\n",
    "X_test_interactions = X_test_processed.copy()\n",
    "\n",
    "# Document the original feature space\n",
    "print(f\"Original feature space: {X_train_processed.shape[1]} features\")\n",
    "\n",
    "# 1. Add interaction between temperature and humidity\n",
    "print(\"Adding temp × humidity interaction feature\")\n",
    "X_train_interactions['atemp_hum_interaction'] = X_train_interactions['atemp'] * X_train_interactions['hum']\n",
    "X_val_interactions['atemp_hum_interaction'] = X_val_interactions['atemp'] * X_val_interactions['hum']\n",
    "X_test_interactions['atemp_hum_interaction'] = X_test_interactions['atemp'] * X_test_interactions['hum']\n",
    "\n",
    "# 2. Add additional interaction for time of day and weather\n",
    "print(\"Adding hour (cyclical) × weather interaction features\")\n",
    "X_train_interactions['hr_sin_weather'] = X_train_interactions['hr_sin'] * X_train_interactions['weathersit_1']\n",
    "X_val_interactions['hr_sin_weather'] = X_val_interactions['hr_sin'] * X_val_interactions['weathersit_1']\n",
    "X_test_interactions['hr_sin_weather'] = X_test_interactions['hr_sin'] * X_test_interactions['weathersit_1']\n",
    "\n",
    "# 3. Add season-related interactions\n",
    "print(\"Adding season × temperature interaction\")\n",
    "for season_col in [col for col in X_train_interactions.columns if 'season' in col]:\n",
    "    X_train_interactions[f'{season_col}_temp'] = X_train_interactions[season_col] * X_train_interactions['atemp']\n",
    "    X_val_interactions[f'{season_col}_temp'] = X_val_interactions[season_col] * X_val_interactions['atemp']\n",
    "    X_test_interactions[f'{season_col}_temp'] = X_test_interactions[season_col] * X_test_interactions['atemp']\n",
    "\n",
    "# Log the expanded feature space\n",
    "print(f\"Expanded feature space: {X_train_interactions.shape[1]} features\")\n",
    "print(f\"Added {X_train_interactions.shape[1] - X_train_processed.shape[1]} new interaction features\")\n",
    "\n",
    "# Function to evaluate model with detailed metrics\n",
    "def evaluate_refined_model(model, X_train, X_val, y_train, y_val, model_name):\n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"\\nTraining refined {model_name} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    train_metrics = {\n",
    "        'MSE': mean_squared_error(y_train, y_train_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'R2': r2_score(y_train, y_train_pred),\n",
    "        'Explained Variance': explained_variance_score(y_train, y_train_pred)\n",
    "    }\n",
    "    \n",
    "    val_metrics = {\n",
    "        'MSE': mean_squared_error(y_val, y_val_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
    "        'MAE': mean_absolute_error(y_val, y_val_pred),\n",
    "        'R2': r2_score(y_val, y_val_pred),\n",
    "        'Explained Variance': explained_variance_score(y_val, y_val_pred)\n",
    "    }\n",
    "    \n",
    "    # Print metrics in a formatted table\n",
    "    print(f\"\\n{model_name} Performance Metrics:\")\n",
    "    print(f\"{'Metric':<20} {'Training':<15} {'Validation':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric in train_metrics.keys():\n",
    "        print(f\"{metric:<20} {train_metrics[metric]:<15.4f} {val_metrics[metric]:<15.4f}\")\n",
    "    \n",
    "    return train_metrics, val_metrics, y_val_pred\n",
    "\n",
    "# Evaluate the refined XGBoost model\n",
    "refined_xgb = tuned_xgb\n",
    "_, refined_metrics, y_val_pred_refined = evaluate_refined_model(\n",
    "    refined_xgb, X_train_interactions, X_val_interactions, y_train, y_val, \"Refined XGBoost\")\n",
    "\n",
    "# Compare with baseline tuned model\n",
    "print(\"\\nIMPROVEMENT ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "for metric in ['MSE', 'RMSE', 'MAE', 'R2', 'Explained Variance']:\n",
    "    if metric in xgb_val_metrics:\n",
    "        baseline = xgb_val_metrics[metric]\n",
    "        refined = refined_metrics[metric]\n",
    "        change = (refined - baseline) / abs(baseline) * 100\n",
    "        \n",
    "        # Determine if the change is positive or negative\n",
    "        if (metric == 'R2' or metric == 'Explained Variance') and change > 0:\n",
    "            status = \"✓ Improved\"\n",
    "        elif (metric != 'R2' and metric != 'Explained Variance') and change < 0:\n",
    "            status = \"✓ Improved\"\n",
    "        else:\n",
    "            status = \"✗ Worsened\"\n",
    "        \n",
    "        print(f\"{metric}: {baseline:.4f} → {refined:.4f} ({change:.2f}%) {status}\")\n",
    "\n",
    "# Visualize the impact of feature engineering\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Feature importance from the refined model\n",
    "plt.subplot(2, 1, 1)\n",
    "feature_importance = pd.Series(refined_xgb.feature_importances_, \n",
    "                               index=X_train_interactions.columns).sort_values(ascending=False)\n",
    "top_features = feature_importance.head(15)\n",
    "sns.barplot(x=top_features.values, y=top_features.index, palette='viridis')\n",
    "plt.title('Top 15 Features After Refinement', fontsize=14)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Plot 2: Actual vs Predicted values with the refined model\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(y_val, y_val_pred_refined, alpha=0.5, color='steelblue')\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "plt.title('Refined Model: Actual vs Predicted Values', fontsize=14)\n",
    "plt.xlabel('Actual', fontsize=12)\n",
    "plt.ylabel('Predicted', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if any of the new interaction features appear in the top 15 important features\n",
    "new_features = [col for col in X_train_interactions.columns if col not in X_train_processed.columns]\n",
    "important_new_features = [feat for feat in top_features.index if feat in new_features]\n",
    "print(\"\\nIMPORTANT NEW INTERACTION FEATURES:\")\n",
    "for i, feat in enumerate(important_new_features, 1):\n",
    "    importance = feature_importance[feat]\n",
    "    rank = list(feature_importance.index).index(feat) + 1\n",
    "    print(f\"{i}. {feat} (Rank: {rank}, Importance: {importance:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training and validation sets\n",
    "X_final_train = pd.concat([X_train_interactions, X_val_interactions])\n",
    "y_final_train = pd.concat([y_train, y_val])\n",
    "\n",
    "# Train final model on combined data\n",
    "final_model = tuned_xgb\n",
    "final_model.fit(X_final_train, y_final_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = final_model.predict(X_test_interactions)\n",
    "\n",
    "print(\"\\nFinal Model Test Performance:\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_test, y_test_pred):.4f}\")\n",
    "print(f\"R2: {r2_score(y_test, y_test_pred):.4f}\")\n",
    "\n",
    "# Feature importance of final model\n",
    "plt.figure(figsize=(12, 8))\n",
    "final_importance = pd.Series(final_model.feature_importances_, index=X_final_train.columns)\n",
    "final_importance.nlargest(10).plot(kind='barh')\n",
    "plt.title('Top 10 Most Important Features in Final Model')\n",
    "plt.show()\n",
    "\n",
    "# Compare with validation performance\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"Validation R2:\", r2_score(y_val, y_val_pred_refined))\n",
    "print(\"Test R2:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate additional statistics for model predictions\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Create analysis for test predictions\n",
    "print(\"=\"*50)\n",
    "print(\"DETAILED MODEL PREDICTION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate error statistics\n",
    "absolute_errors = np.abs(y_test - y_test_pred)\n",
    "percentage_errors = np.abs((y_test - y_test_pred) / y_test) * 100\n",
    "squared_errors = (y_test - y_test_pred) ** 2\n",
    "\n",
    "# Basic error statistics\n",
    "print(\"\\nError Statistics:\")\n",
    "print(f\"Mean Absolute Error (MAE): {np.mean(absolute_errors):.2f}\")\n",
    "print(f\"Median Absolute Error: {np.median(absolute_errors):.2f}\")\n",
    "print(f\"90th Percentile of Absolute Error: {np.percentile(absolute_errors, 90):.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {np.mean(percentage_errors):.2f}%\")\n",
    "print(f\"Root Mean Square Error (RMSE): {np.sqrt(np.mean(squared_errors)):.2f}\")\n",
    "\n",
    "# Calculate prediction statistics\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(f\"Mean of Actual Values: {np.mean(y_test):.2f}\")\n",
    "print(f\"Mean of Predicted Values: {np.mean(y_test_pred):.2f}\")\n",
    "print(f\"Standard Deviation of Actual Values: {np.std(y_test):.2f}\")\n",
    "print(f\"Standard Deviation of Predicted Values: {np.std(y_test_pred):.2f}\")\n",
    "print(f\"Min of Actual Values: {np.min(y_test):.2f}\")\n",
    "print(f\"Min of Predicted Values: {np.min(y_test_pred):.2f}\")\n",
    "print(f\"Max of Actual Values: {np.max(y_test):.2f}\")\n",
    "print(f\"Max of Predicted Values: {np.max(y_test_pred):.2f}\")\n",
    "\n",
    "# Calculate over/under prediction statistics\n",
    "over_predictions = y_test_pred > y_test\n",
    "under_predictions = y_test_pred < y_test\n",
    "exact_predictions = y_test_pred == y_test\n",
    "\n",
    "print(\"\\nOver/Under Prediction Analysis:\")\n",
    "print(f\"Over-predictions: {np.sum(over_predictions)} ({np.mean(over_predictions)*100:.1f}%)\")\n",
    "print(f\"Under-predictions: {np.sum(under_predictions)} ({np.mean(under_predictions)*100:.1f}%)\")\n",
    "print(f\"Exact predictions: {np.sum(exact_predictions)} ({np.mean(exact_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Calculate error distribution by actual value ranges\n",
    "def error_by_range(y_true, y_pred, bin_edges):\n",
    "    bin_indices = np.digitize(y_true, bin_edges) - 1\n",
    "    bin_count = len(bin_edges) - 1\n",
    "    bin_mae = np.zeros(bin_count)\n",
    "    bin_mape = np.zeros(bin_count)\n",
    "    bin_counts = np.zeros(bin_count)\n",
    "    \n",
    "    for i in range(bin_count):\n",
    "        mask = bin_indices == i\n",
    "        if np.sum(mask) > 0:\n",
    "            bin_mae[i] = np.mean(np.abs(y_true[mask] - y_pred[mask]))\n",
    "            bin_mape[i] = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
    "            bin_counts[i] = np.sum(mask)\n",
    "    \n",
    "    return bin_mae, bin_mape, bin_counts\n",
    "\n",
    "# Create bin edges based on the distribution of actual values\n",
    "bin_edges = np.percentile(y_test, [0, 25, 50, 75, 100])\n",
    "bin_mae, bin_mape, bin_counts = error_by_range(y_test, y_test_pred, bin_edges)\n",
    "\n",
    "print(\"\\nError Analysis by Actual Value Range:\")\n",
    "print(f\"{'Range':<20} {'Count':<10} {'MAE':<10} {'MAPE (%)':<10}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(len(bin_edges)-1):\n",
    "    print(f\"{bin_edges[i]:.0f}-{bin_edges[i+1]:.0f}:{' ':>10} {bin_counts[i]:<10.0f} {bin_mae[i]:<10.2f} {bin_mape[i]:<10.2f}\")\n",
    "\n",
    "# Prediction accuracy by quartile\n",
    "quartiles = np.percentile(y_test, [25, 50, 75])\n",
    "q1_mask = y_test <= quartiles[0]\n",
    "q2_mask = (y_test > quartiles[0]) & (y_test <= quartiles[1])\n",
    "q3_mask = (y_test > quartiles[1]) & (y_test <= quartiles[2])\n",
    "q4_mask = y_test > quartiles[2]\n",
    "\n",
    "print(\"\\nPrediction Performance by Quartile:\")\n",
    "print(f\"Q1 (≤{quartiles[0]:.2f}): R² = {r2_score(y_test[q1_mask], y_test_pred[q1_mask]):.4f}, MAE = {mean_absolute_error(y_test[q1_mask], y_test_pred[q1_mask]):.2f}\")\n",
    "print(f\"Q2 ({quartiles[0]:.2f}-{quartiles[1]:.2f}): R² = {r2_score(y_test[q2_mask], y_test_pred[q2_mask]):.4f}, MAE = {mean_absolute_error(y_test[q2_mask], y_test_pred[q2_mask]):.2f}\")\n",
    "print(f\"Q3 ({quartiles[1]:.2f}-{quartiles[2]:.2f}): R² = {r2_score(y_test[q3_mask], y_test_pred[q3_mask]):.4f}, MAE = {mean_absolute_error(y_test[q3_mask], y_test_pred[q3_mask]):.2f}\")\n",
    "print(f\"Q4 (>{quartiles[2]:.2f}): R² = {r2_score(y_test[q4_mask], y_test_pred[q4_mask]):.4f}, MAE = {mean_absolute_error(y_test[q4_mask], y_test_pred[q4_mask]):.2f}\")\n",
    "\n",
    "# Visualize error distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Histogram of errors\n",
    "plt.subplot(2, 2, 1)\n",
    "errors = y_test - y_test_pred\n",
    "sns.histplot(errors, kde=True, bins=30)\n",
    "plt.title('Distribution of Prediction Errors', fontsize=12)\n",
    "plt.xlabel('Error (Actual - Predicted)')\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot of error vs actual value\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(y_test, errors, alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Error vs Actual Value', fontsize=12)\n",
    "plt.xlabel('Actual Value')\n",
    "plt.ylabel('Error')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Box plot of absolute errors by value quartile\n",
    "plt.subplot(2, 2, 3)\n",
    "quartile_names = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "quartile_errors = [absolute_errors[q1_mask], absolute_errors[q2_mask], \n",
    "                   absolute_errors[q3_mask], absolute_errors[q4_mask]]\n",
    "plt.boxplot(quartile_errors, labels=quartile_names)\n",
    "plt.title('Absolute Errors by Value Quartile', fontsize=12)\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: Actual vs predicted with perfect prediction line\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(y_test, y_test_pred, alpha=0.5)\n",
    "min_val = min(np.min(y_test), np.min(y_test_pred))\n",
    "max_val = max(np.max(y_test), np.max(y_test_pred))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "plt.title('Actual vs Predicted Values', fontsize=12)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(checkpoint_dir, 'prediction_analysis.png'))\n",
    "plt.show()\n",
    "\n",
    "# Calculate prediction metrics for specific subgroups\n",
    "# For example, by season, hour of day, or weather condition\n",
    "print(\"\\nPerformance by Weather Condition:\")\n",
    "for weather in np.unique(X_test['weathersit']):\n",
    "    weather_mask = X_test['weathersit'] == weather\n",
    "    if np.sum(weather_mask) > 0:\n",
    "        mae = mean_absolute_error(y_test[weather_mask], y_test_pred[weather_mask])\n",
    "        r2 = r2_score(y_test[weather_mask], y_test_pred[weather_mask])\n",
    "        print(f\"Weather {weather}: Count = {np.sum(weather_mask)}, MAE = {mae:.2f}, R² = {r2:.4f}\")\n",
    "\n",
    "# Calculate time-based prediction performance\n",
    "print(\"\\nPerformance by Hour Range:\")\n",
    "morning = (X_test['hr'] >= 6) & (X_test['hr'] < 12)\n",
    "afternoon = (X_test['hr'] >= 12) & (X_test['hr'] < 18)\n",
    "evening = (X_test['hr'] >= 18) & (X_test['hr'] < 22)\n",
    "night = (X_test['hr'] >= 22) | (X_test['hr'] < 6)\n",
    "\n",
    "print(f\"Morning (6-11): MAE = {mean_absolute_error(y_test[morning], y_test_pred[morning]):.2f}, R² = {r2_score(y_test[morning], y_test_pred[morning]):.4f}\")\n",
    "print(f\"Afternoon (12-17): MAE = {mean_absolute_error(y_test[afternoon], y_test_pred[afternoon]):.2f}, R² = {r2_score(y_test[afternoon], y_test_pred[afternoon]):.4f}\")\n",
    "print(f\"Evening (18-21): MAE = {mean_absolute_error(y_test[evening], y_test_pred[evening]):.2f}, R² = {r2_score(y_test[evening], y_test_pred[evening]):.4f}\")\n",
    "print(f\"Night (22-5): MAE = {mean_absolute_error(y_test[night], y_test_pred[night]):.2f}, R² = {r2_score(y_test[night], y_test_pred[night]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(checkpoint_dir, 'bike_rental_final_model.pkl')\n",
    "with open(final_model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# Also save the feature preprocessor for future predictions\n",
    "preprocessor_path = os.path.join(checkpoint_dir, 'feature_preprocessor.pkl')\n",
    "with open(preprocessor_path, 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "print(f\"Feature preprocessor saved to {preprocessor_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
